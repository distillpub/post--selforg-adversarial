<d-article>
{% include contents.html %}
{% include nextPrev.html %}
<p style="color:red;">This is a draft. Please do not share it further. If you want to comment or suggest changes, please ask access to this <a href="https://docs.google.com/document/d/1u-2Y1qiQwhxBEth7xbe957wSB7zkP7wG3XgejtAwMmU/edit?usp=sharing">Google Document</a>.</p>

<p>Biology is transitioning from a focus on mechanism (what is required for the system to work) to a focus on information (what algorithm is sufficient to implement adaptive behavior). Advances in machine learning represent an exciting and still largely untapped source of inspiration and tools to assist the biological sciences. The Growing Neural Cellular Automata <d-cite key="mordvintsev2020growing"></d-cite> and Self-classifying MNIST Digits <d-cite key="randazzo2020self-classifying"></d-cite> articles in this thread presented the Neural Cellular Automata (Neural CAs) model and showed how it can be trained end-to-end differentiably to accomplish self-organising task such as pattern growth and digit self-classification. The resulting models were robust to various kinds of perturbations: the growing CAs expressed regenerative capabilities to damage; the MNIST CAs were responsive to change in the underlying digits, triggering reclassification whenever necessary. These computational frameworks represent quantitative models with which to understand important biological phenomena, such as scaling of single cell behavior rules into reliable organ-level anatomies. The latter is a kind of anatomical homeostasis, achieved by feedback loops that must recognize deviations from a correct target morphology and progressively reduce anatomical error.</p>

<p>In a complex system, whether biological, technological, or social, how can we discover signaling events that will alter system-level behavior in desired ways? Even when the rules governing the behavior of subunits are known, this difficult inverse problem is at the heart of many barriers in biomedicine, robotics, and other fields of importance to society.</p>

<p>In this work, we <i>train adversaries </i>whose goal is to reprogram the CAs into doing something other than what they were trained to do. In order to understand what kind of signaling or change in a system could alter system-level behavior of our CAs, it is important to understand how these CAs are constructed and where both local and global information resides within their aggregate configurations.</p>

<p>The system-level behavior of Neural CAs is affected by:</p>
<ul><li><strong>Each state within a cell. </strong>States store information which is used for both diversification among cell behaviours and for communication with neighboring cells.</li>
<li><strong>The model parameters. </strong>These are the actuators and are shared by every Neural CA of the same family. The model parameters can be seen as <i>the way the system works</i>.</li>
<li><strong>The perceptive field&#x2F;system. </strong>This is how cells perceive their environment. In Neural CAs, we always restrict the perceptive field to be of their 8 nearest neighbors and themselves. The way they are perceived is different between the Growing CA and MNIST CA. The Growing CA perceptive field is fixed during training, while the MNIST CA perceptive field is part of the model parameters.</li></ul>
<p>Perturbing any of these components will result in system-level behavioral changes.</p>

<p>We will explore two kinds of adversarial attacks: injecting some adversarial CA into the collective pretrained models; and perturbing the global state of the cells with some adversarial operation.</p>

<p>For the first type of adversarial attacks we will train a new CA model that, when placed in an environment with the original models described in the previous articles, is able to hijack the behavior of the collective CAs. Therefore, we are injecting CAs with different <i>model parameters</i> into the system. Numerous forms of biological hijacking are known, including viruses that take over genetic and biochemical information flow <d-cite key="pmid32457704"></d-cite>, bacteria that take over physiological control mechanisms <d-cite key="pmid21278760"></d-cite> and even regenerative morphology of whole bodies <d-cite key="pmid32439577"></d-cite>, and fungi and toxoplasma that modulate host behavior <d-cite key="pmid30109417"></d-cite>. Especially fascinating are the many cases of non-cell-autonomous signaling developmental biology and cancer, showing that some cell behaviors can significantly alter host properties both locally and at long range. For example, bioelectrically-abnormal cells can trigger metastatic conversion in an otherwise normal body (with no genetic defects) <d-cite key="pmid23196890"></d-cite>, while management of bioelectrical state in one area of the body can suppress tumorigenesis on the other side of the organism <d-cite key="pmid24830454"></d-cite>. Similarly, amputation damage in one leg initiates changes to ionic properties of cells in the contralateral leg <d-cite key="pmid30126906"></d-cite>, while the size of the developing brain is in part dictated by the activity of ventral gut cells <d-cite key="pmid26198142"></d-cite>. All of these phenomena underlie the importance of understanding how cell groups make collective decisions, and how those tissue-level decisions can be subverted by the activity of a small number of cells. It is essential to develop quantitative models of such dynamics, in order to drive meaningful progress in regenerative medicine that controls system-level outcomes top-down, where cell- or molecular-level micromanagement is infeasible <d-cite key="pmid27807271"></d-cite>.</p>

<p>The second type of adversarial attacks will interact with previously trained growing CA models by <i>perturbing the states within cells</i>. We will apply a global state perturbation to all alive cells. This can be seen as inhibiting or enhancing combinations of state values, in turn hijacking proper communications among cells and within the cell's own states. Models like this represent not only ways of thinking about adversarial relationships in nature (such as parasitism and evolutionary arms races of genetic and physiological mechanisms), but also a roadmap for the development of regenerative medicine strategies. Next-generation biomedicine will need computational tools for inferring minimal, least-effort interventions that can be applied to biological systems to predictively change their large-scale anatomical and behavioral properties.</p>
<h2 id='adversarial-mnist-cas'>Adversarial MNIST CAs</h2>
          <a href="https://colab.research.google.com/drive/14xFg14OUbFI6es1LMuIoDKmMh8eGDJJY" class="colab-root">Try in a <span class="colab-span">Notebook</span></a>

<p>As a brief recap, the Self-classifying MNIST digits task consisted in placing CAs on a plane forming the shape of an MNIST digit. The cells had to communicate among themselves in order to achieve a total agreement as to which digit they formed.</p>
<figure>
<img src='images/local_global_figure.svg' style='width: 650px'>
<figcaption>Diagram showing the local vs global information available in the cell collective. <br> (a) Local information neighbourhood - each cell can only observe itself and its neighbors' states, or the absence of neighbours.<br> (b) Globally, the cell collective aggregates information from all parts of itself. <br> (c) It is able to distinguish certain shapes that compose a specific digit (3 in the example). </figcaption>
</figure>
<p>Below we show example classifications of the model trained in the previous article.</p>

<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/mnist_orig_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
The original model behavior on test data. We highlight mistakes in classification with a red background.</figcaption>
</figure>

<p>In this experiment, <strong>the goal is to create adversarial CAs that can hijack the global cells' classifications to always be an 8</strong>. We take the CA model trained on the previous article and freeze its parameters. We then train a new set of CAs whose model is identical to the specs of the frozen model but is randomly initialized. The training regime is mostly identical to the one used for training the self-classifying MNIST digits CAs. There are three important differences:</p>
<ul><li>Regardless of what the actual digit is, we consider <i>the correct classification to always be an 8</i>.</li>
<li>For each batch and each pixel, the CAs are randomly chosen to be either the pretrained model or the new adversarial one. We use a 10% chance of placing an adversarial CA instead of the pretrained one.</li>
<li>We train only the adversarial CA parameters and keep the pretrained model frozen.</li></ul>

<p>Note that this adversarial attack only modifies a very small percentage of the overall system, and the goal is to affect every alive cell. Therefore, these adversaries have to somehow learn to communicate deceiving information that cause wrong classifications and further cascades in the propagation of deceiving information by unaware cells. The unaware cells' parameters cannot be changed so the only means of attack by the adversaries is to cause a change in the cells' states. Cells' states are responsible for communication and diversification.</p>

<p>The task is remarkably simple to optimize, approaching convergence in as little as 2000 training steps. The adversaries are constantly communicating with their neighbors to keep them convinced of the wrong classification. We can see that by visualising what happens when we remove the adversaries. While some digits don't recover, most of them self-correct to the right classification. Here we show a few test examples where we place the adversaries after 200 steps and remove them after 200 more steps.</p>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/mnist_10perc_adv_reset_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
We place the adversaries (red pixels) after 200 steps and remove them after 200 more steps. Most digits recover, but not all.We highlight mistakes in classification with a red background.</figcaption>
</figure>
<p>While we trained the adversaries with a 10% distribution, we can observe that very often we need significantly fewer adversaries to steer the entire classification towards an 8. Here is a test run with 1% of cells being adversaries.</p>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/mnist_1perc_adv_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 1% of the cells (red pixels).We highlight mistakes in classification with a red background.</figcaption>
</figure>
<p>We created a demo playground where the reader can draw digits and place adversaries with surgical precision. We encourage the reader to play with the demo to get a sense on how easily innocent cells can get swayed towards the wrong outcomes.</p>
<iframe id="mnistDemoIframe" src="demo.html" scrolling="no" frameborder="0"
    style="position: relative; height: 100px; width: 100px;"
></iframe>
<script>
    var iframe = document.getElementById("mnistDemoIframe");
    
    iframe.onload = function(){
        iframe.style.height = iframe.contentWindow.document.body.scrollHeight + 'px';
        iframe.style.width = '100%';
    };
    window.onresize = iframe.onload;
</script>
<h2 id='adversarial-injections-for-growing-cas'>Adversarial Injections for Growing CAs</h2>
<a href="https://colab.research.google.com/drive/1N2V8auh_Hsl6kL4FOK18_OS4BakU1n4t#scrollTo=ByHbsY0EuyqB" class="colab-root">Try in a <span class="colab-span">Notebook</span></a>
<p>One question we may ask is whether the same kind of adversarial attack that was successful for the MNIST CA would work for the Growing CA model too. The Growing CA models' goals are to be able to grow a complex image pattern from a small seed, and having its result be persistent over time and robust to significant perturbations. In this article, we will focus on the lizard pattern.</p>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="450px" preload="auto">
      <source src="images/lizard_complete_video.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
The target CA to hijack.</figcaption>
</figure>
<p>The new goal is to have some adversarial CAs change the global configuration of all the CAs, including the previously trained and now frozen lizard CAs. We showcase two new targets for the adversaries to generate: a tailless lizard and a red lizard. </p>
<figure>
<img src='images/lizard_new_targets_exp2.png' style='width: 450px'>
<figcaption>List of the desired mutations we want to apply.</figcaption>
</figure>

<p>These targets have very different properties: </p>
<ul><li><strong>Red lizard:</strong> converting a lizard from green to red would show a global change in the behaviour of the resulting CAs. This behavior is not present as is in the current combination of states&#x2F;model symbiosis. The adversaries have to then find a way to fool other cells into doing things they have never done before (create a lizard shape but colored in red).</li>
<li><strong>Tailless lizard:</strong> having a severed tail is a more localized change that requires some cells to be fooled into diversifying themselves in the wrong way: cells need to believe to be at the edge of the pattern even though they are where the tail is supposed to grow.</li></ul>
<p>Just like in the previous experiment, our adversaries can only affect, indirectly, the states of the original cells.</p>

<p>We first train adversaries for the tailless target with a 10% chance for each cell to be a virus. Note how we prohibit cells to be viruses if they are outside the target pattern: the tail contains no viruses.</p>

<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/lizard_no_tail_virus_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 10% of the cells.</figcaption>
</figure>

<p>The video above shows 6 different runs of the same model with different adversaries placements. The results vary considerably: sometimes the virus successfully gets rid of the tail, sometimes the tail is only shrunk but not completely removed, and other times the pattern becomes unstable. It is also important to note that training these adversaries required much longer times to achieve convergence and the acquired convergence is of a worse quality than what achieved for the MNIST CA adversarial experiment.</p>

<p>This difference in quality is exacerbated when trying to fit the red lizard pattern. In this case, using only 10% of adversaries results in a complete failure: the original cells are unaffected by the adversaries. Some readers may wonder whether the frozen CAs have the capability of producing red cells at all, since there is no red in the original target, and suspect this was an impossible task to begin with. However, we note that the frozen model knows how to make red, because there are instances where a cell has to color itself white, and therefore has to know how to generate both green, red, and blue.</p>

<p>To test how harder it is to have a "red virus" hijack the lizard pattern, we increased the percentage of the virus to as much as 60% of the total number of cells.</p>

<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/lizard_red_virus_single_run.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 60% of the cells. At 500 steps, we stop the image and show only cells that are from the original model.</figcaption>
</figure>
<p>In the video above we can see how, at least at the beginning, 60% of adversaries are actually capable of coloring the lizard red. Take particular notice of the "step 500"<d-footnote> The still-image of the video is on step 500, and the video stops for a bit more than a second on step 500.</d-footnote>, where we mask out all the adversary cells and only show the original cells. There, we can see how some original cells are colored in red. This is proof that the adversaries successfully managed to steer neighboring cells to color themselves red, where needed.</p>

<p>However, the model is far from perfect: if left be for a while, it diverges and new lizards spawn. Moreover, the percentage of cells, being 60%, make it so the adversaries are heavily dependent on having a majority of cells being adversaries, and if we were to place a smaller percentage of adversaries, such as 20-30%, the configuration would be unstable.</p>

<p>In comparison to the results of the previous experiment, the Growing CA model shows a much greater resistance to adversarial perturbation than those of the MNIST CA. One notable difference between the two models is that the MNIST CAs have to change their opinion (digit classification) based on the information that they receive from their neighbors. This is because at any time the underlying digit may change in shape even if in a specific position there would be no apparent change. For instance, imagine the case of a "1" turning into a "7": from the point of view of the cells in the lower part of the digit, there is no change, yet the total digit is a 7 now. This may make the MNIST CAs more susceptible to individuals' signals rather than what happens in Growing CAs, where the totality of cells is responsible for generating a given shape and there is never a moment where cells have to reconfigure themselves to regenerate something different than before.</p>

<p>Following this hypothesis, we suspect that more general-purpose Growing CAs that have observed different target patterns during training are more likely to be susceptible to adversarial attacks.</p>
<h2 id='perturbing-the-states-of-growing-cas'>Perturbing the states of Growing CAs</h2>
<a href="https://colab.research.google.com/drive/1N2V8auh_Hsl6kL4FOK18_OS4BakU1n4t#scrollTo=JaITnQv0k1iY" class="colab-root">Try in a <span class="colab-span">Notebook</span></a>
<p>We observed that it was hard to fool Growing CAs into changing their morphology by placing viruses inside them. These adversaries had to devise complex and local behaviors that would cause the states of original cells nearby, and ultimately globally, to change their morphology.</p>
<p>In this section, we explore how we can circumvent this added difficulty of indirect behavioral change from local interactions, by performing global state changes.</p>
<p>We again focus on Growing CAs and we choose the lizard pattern again for illustrative purposes. Every cell of a Growing CA consists of a group of 16 states, some of them phenotypical (RGBA states) and the remaining 12 of arbitrary purposes, generally used for storing and communicating information. As we emphasized above, because the states of every cell are used to understand what to do and communicate with each other, we can perturb the states of these cells to hijack the overall system in certain ways (discovery of such perturbation strategies is a key goal of biomedicine and synthetic morphology). There are a variety of ways we can perform state perturbations. We will focus on <i>global state perturbations</i>, defined as perturbations that are applied on every alive cell in every time step (analogous to "systemic" biomedical interventions, that are given to the whole organism (e.g., a chemical take internally), as opposed to highly localized delivery systems). The new goal is to discover a certain type of global state perturbation that results in a stable new pattern.</p>
<figure>
<img src='images/figure_2.svg' style='width: 650px'>
<figcaption>Diagram showing some possible stages for perturbing a lizard pattern. (a) We start from a seed that grows into a lizard (b) Fully converged lizard. (c) We apply a global state perturbation at every step. As a result, the lizard loses its tail. (d) We stop perturbing the state. We observe the lizard immediately grows back its tail.</figcaption>
</figure>

<p>We show 6 target patterns: the tailless and red lizard from the previous experiment, plus a blue lizard and lizards with other severed limbs and severed head.</p>
<figure>
<img src='images/mutations_mosaic.jpeg' style='width: 450px'>
<figcaption>Mosaic of the desired mutations we want to apply.</figcaption>
</figure>
<p>We decided to experiment with a very simple kind of global state perturbation: applying a symmetric $16\times16$ matrix multiplication $A$ to every alive cell at every step<d-footnote> In practice, we also clip the state of cells such that they stay bounded within $[-3, +3]$. This is a minor detail and it stabilizes results minimally. Everything discussed here applies even if we were to not clip the space.</d-footnote>. To give insight on why we chose this, an even simpler "state addition" mutation would be insufficient because our models are technically unbounded, and very often we would want to suppress something by setting it to zero - something impossible to do with constant state additions. However, matrix multiplications have the possibility of amplifying&#x2F;suppressing combinations of states. As to why we force the matrix to be symmetric will be clear later on.</p>

<p>We initialize $A$ as the identity matrix $I$ and train $A$ just like we would train traditional Growing CAs, with the following differences:</p>
<ul><li>We perform a global state perturbation through $A$ at every step.</li>
<li>The CA parameters are frozen and train $A$ only.</li>
<li>We consider initial image configurations to be both a seed and a fully grown lizard (as opposed to the Growing CA article, where initial configurations were seeds only).</li></ul>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/mutations_mosaic_video.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of applying the trained perturbations.</figcaption>
</figure>


<p>The video above shows how this kind of training results is successfully discovering global state perturbations to change the target patterns to a desired variation of it. We show what happens when we stop perturbing the states (an out-of-training situation) at step 500 throughout step 1000, and then reapply the mutation. This way we observe how both from a seed and from a grown pattern applying these perturbations is successful, and that the original CAs easily recover from this state perturbation once it goes away. This last result is perhaps not surprising after we observed how robust growing CA models are in general.</p>

<p>We can also see how not all perturbations are equally effective. In particular, the headless perturbation is the least successful as it results in a loss of other details across the whole lizard pattern such as the white coloring on its back. We hypothesize that the best this simple perturbation managed to find was suppressing a "structure" that contained both the head and the white coloring. This may be related to the concept of differentiation and distinction of biological organs. Predicting what kinds of perturbations would be harder or impossible to be made before trying them out empirically is still an open research question. On the other hand, a variant of this kind of synthetic analysis might help with defining higher order structures within systems.</p>
<h3 id='directions-and-compositionality-of-perturbations'>Directions and compositionality of perturbations</h3>
<p>Let us go back to our choice of using a symmetric matrix for representing global state perturbations. Every complex symmetric matrix $A$ can be diagonalized as follows: </p>
<p>$A &#x3D; Q \Lambda Q^\intercal$</p>
<p>where $\Lambda$ is the diagonal eigenvalues matrix and $Q$ is the unitary matrix of its eigenvectors. Another way of seeing this now becomes as applying a change of basis transformation, scaling each single component proportional to the eigenvalues, and then changing back to the original basis. This should also give a clearer intuition on how easy it is to suppress&#x2F;amplify combinations of states. Moreover, we can now imagine what would happen if all the eigenvalues were to be one. In that case, we would naturally have $Q I Q^\intercal &#x3D; I$ resulting in a noop (no change): the lizard would grow as if no perturbation would be performed. We now can decompose $Q \Lambda Q^\intercal &#x3D; Q (D + I) Q^\intercal$ where D is the <i>perturbation direction </i>($\Lambda - I$) in the "eigenvalue space". Suppose now we have a coefficient $k$ to scale D: $A_k &#x3D; Q (kD + I) Q^\intercal$. If $k&#x3D;1$, we have the original perturbation $A$ and $k&#x3D;0$, we have the noop $I$. One natural question would be whether we can explore this direction and discover meaningful perturbations. Since </p>
<p>$A_k &#x3D; Q (kD + I) Q^\intercal &#x3D; k A + (1-k) I$ </p>
<p>we do not even have to compute eigenvalues and eigenvectors and we can simply scale $A$ and $I$ accordingly.</p>

<p>Let us then take the tailless perturbation and see what happens as we vary k:</p>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/tail_direction_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of following the direction of the tail perturbation.</figcaption>
</figure>
<p>As we descend from $k&#x3D;1$ to 0 we can clearly see the tail forming more and more complete. Surprisingly, if we proceed towards negative k values, so to the opposite direction of what we trained for, the lizard grows a longer tail. Unfortunately, the further away we go, the more cancerogenous the system becomes and eventually the lizard explodes. This cancerogenic behaviour is likely due to the fact that some of the perturbations applied on the states are also responsible for changing the homeostatic regulation of the system, making some cells die out&#x2F;grow in different ways than before. When we go the opposite direction, we are both going towards a "longer tail" regimen, but also changing these regulatory processes and eventually "cancer" appears.</p>

<p><strong>Can we perform multiple individually trained perturbations at the same time?</strong> </p>
<p>Suppose we had two perturbations $A$ and $B$ and their eigenvectors were the same, or more realistically not that different from one another. Then, $A_k &#x3D; Q (k_A D_A + I) Q^\intercal$ and $B_k &#x3D; Q (k_B D_B + I) Q^\intercal$. </p>
<p>In that case, it could be that </p>
<p>$comb(A_k, B_k) &#x3D; Q(k_A D_A + k_B D_B + I)Q^\intercal &#x3D; k_A A + k_B B + (1 - k_A - k_B)I$ </p>
<p>would result in something meaningful. At the very least, if $A &#x3D; B$, setting $k_A &#x3D; k_B &#x3D; 0.5$ would result in exactly the same perturbation.</p>
<p>Since $D_A$ and $D_B$ are effectively a displacement from the identity $I$ and we have empirically observed how given any trained displacement $D_A$, for $0 \leq k_A \leq 1$ adding $k_A D_A$ results in a stable perturbation, we can hypothesize that as long as we have two perturbations whose positive directions $k$ are $k_A + k_B \leq 1$, this could result in a stable perturbation. Picture it as interpolating stable perturbations with the direction coefficients.</p>
<p>In practice the eigenvectors are also different so the combination results are likely going to be worse the more different the respective eigenvector bases are.</p>
<p>Now let us take a look at two perturbations, tailless and no leg lizards, and interpolate their direction coefficients by keeping their sums equal to 1:</p>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/lizard_combinations_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of composing two trained perturbations while keeping the sum of k=1.</figcaption>
</figure>
<p>We can see how it seems to work, but it has some side effects such as the whole pattern sliding up. Similar results happen with other combinations of perturbations. Now, what would happen if we removed the restriction of the sum of k equal to 1, and instead added both perturbations entirely? We know that if the two perturbations were the same, we would end up going twice as far away from the identity perturbation, and in general we expect the variance of these perturbations to increase. Effectively, this means going further and further away from the stable perturbations discovered during training. This should mean that there are more unwanted changes that can disrupt the CAs the higher the sum of k are.</p>
<p>We now show what happens when we combine the tailless and the no leg lizard perturbations at their fullest. Note that when we set both k to 1, the resulting perturbation is equal to the sum of the two perturbations minus an identity matrix.</p>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="320px" preload="auto">
      <source src="images/combination_of_mutations.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of composing two trained perturbations.</figcaption>
</figure>
<p>Surprisingly, the resulting pattern is almost like we would desire it to be. It is evidently not perfect as it is sliding upwards even faster than the other combinations we tried, showing it's even less stable than them.</p>

<p>The arguments we used can be generalized to any arbitrary number of perturbations. We created a small playground that allows the reader to input their desired combinations. Empirically, we were surprised by how many of these combinations result in good perturbations and it appears true that keeping k bound to 1 results in generally more stable patterns. We also observed how going the opposite direction from a perturbation is usually much more unstable.</p>
<p>Below there is a playground to experiment with these compositional perturbations.</p>
<iframe id="mutatingDemo" src="mutating_gca_demo.html" scrolling="no" frameborder="0"
    style="position: relative; height: 800px; width: 100%;" onload=resizeIframe();
></iframe>
<h2 id='related-work'>Related work</h2>
<p>This work is inspired by Generative Adversarial Networks (GANs) <d-cite key="goodfellow2014generative"></d-cite>. While with GANs it is typical to cotrain pairs of models, in this work we froze the original CAs and trained the adversaries only. This is to the greatest degree inspired by the seminal work <i>Adversarial Reprogramming of Neural Networks </i><d-cite key="elsayed2018adversarial"></d-cite>.</p>
<p>The kinds of state perturbations performed in this article can be seen as targeted latent state manipulations. Word2vec <d-cite key="mikolov2013efficient"></d-cite> shows how latent vector representations can have compositional properties and Fader Networks <d-cite key="lample2018fader"></d-cite> show similar behaviors for image processing. Both of these works and their related work were of great inspiration to us.</p>
<h3 id='influence-maximization'>Influence maximization</h3>
<p>Adversarial cellular automata have parallels to the field of influence maximization. Influence maximization involves the optimal way to spread information, and are often defined on arbitrary graphs, commonly social graphs. They are used to model a wide variety of real-world applications involving information spread in a graph. A common setting is that each vertex in a graph has a binary state, which will change if and only if a sufficient fraction of its neighbours&#39; states switch. Examples of such models are social influence maximization (maximally spreading an idea in a network of people), contagion outbreak minimization (minimizing the spread of a disease in a network of people) and cascade modeling (when small perturbations to a system bring about a larger &#39;phase change&#39;). At the time of writing this article, for instance, contagion minimization is a model of particular interest. NCA are a graph - each cell is a vertex and has edges to its eight neighbours, through which it can pass information. This graph and message structure is significantly more complex than the typical setup underlying much of the research in influence maximization, because NCA cells pass vector-valued messages and have a complex update rule for their internal states, whereas these graphs typically consist of more simple binary cells states and threshold functions on edges determining whether a node has switched states. Many concepts from the field could be applied and are of interest, however.</p>

<p>For example, in this work, we&#39;ve made an assumption that our adversaries can be positioned anywhere in a structure to elicit a response. A common focus of investigation in influence maximization problems is deciding <i>which</i> nodes in a graph will result in maximal influence on the graph. This problem isn&#39;t always analytically tractable, and can involve simulations. Future work on adversarial NCA is applying techniques from influence maximization to finding the optimal placement of adversarial cells.</p>
<h2 id='discussion'>Discussion</h2>
<p>This article showed two different kinds of adversarial attacks that Neural CAs are susceptible to.</p>

<p>The adversarial injections of CAs in frozen Self-classifying MNIST digit CAs showed how a system of agents that are heavily reliant on the passing of information among each other is easily swayed by deceitful signaling. This problem is routinely faced by biological systems, which face hijacking of behavioral, physiological, and morphological regulatory mechanisms by parasites and other agents in the biosphere with which they compete.  Future work in this field of computer technology can benefit from research on biological communication mechanisms to understand how cells maximize reliability and fidelity of inter- and intra-cellular messages required to implement adaptive outcomes.  </p>

<p>The adversarial injection attack was much less effective against Growing CAs and resulted in overall unstable CAs. This dynamic is also of importance to the scaling of control mechanisms (swarm robotics and nested architectures): a key step in "multicellularity" (joining together to form larger systems from sub-agents <d-cite key="levin2019theComp"></d-cite>) is informational fusion, which makes it difficult to identify the source of signals and memory engrams. An optimal architecture would need to balance the need for validating control messages with a possibility of flexible merging of subunits, which wipes out metadata about the specific source of informational signals. Likewise, the ability to respond successfully to novel environmental challenges is an important goal for autonomous artificial systems, which may import from biology strategies that optimize tradeoff between maintaining a specific set of signals and being flexible enough to establish novel signaling regimes when needed.</p>

<p>The global state perturbation experiment on Growing CAs shows how it is still possible to hijack these CAs towards stable out-of-training configurations and how these kinds of attacks are somewhat composable in a similar way to how embeddings spaces are manipulable in the natural language processing and computer vision fields <d-cite key="mikolov2013efficient,lample2018fader"></d-cite>. However, this experiment failed to discover stable out-of-training configurations <i>after the perturbation was lifted</i>. We hypothesize that this is partially due to the extreme regeneration capabilities of the pretrained CAs and that different models could be less capable of recovery from arbitrary perturbations.</p>
</d-article>
<d-appendix>
    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
</d-appendix>
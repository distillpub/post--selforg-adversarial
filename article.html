<d-article>
{% include contents.html %}
{% include nextPrev.html %}

<p style="color:rgba(0,0,0,0.3); font-size:10px"> This article makes strong use of colors in figures and demos. Click <a href="#colorwheel">here</a> to adjust the color palette.</p>

<p>Biology is transitioning from a focus on mechanism (what is required for the system to work) to a focus on information (what algorithm is sufficient to implement adaptive behavior). Advances in machine learning represent an exciting and largely untapped source of inspiration and tooling to assist the biological sciences. Growing Neural Cellular Automata <d-cite key="mordvintsev2020growing"></d-cite> and Self-classifying MNIST Digits <d-cite key="randazzo2020self-classifying"></d-cite> introduced the Neural Cellular Automata (Neural CA) model and demonstrated how tasks requiring self-organisation, such as pattern growth and self-classification of digits, can be trained in an end-to-end, differentiable fashion. The resulting models were robust to various kinds of perturbations: the growing CAs expressed regenerative capabilities to damage; the MNIST CA were responsive to change in the underlying digits, triggering reclassification whenever necessary. These computational frameworks represent quantitative models with which to understand important biological phenomena, such as scaling of single cell behavior rules into reliable organ-level anatomies. The latter is a kind of anatomical homeostasis, achieved by feedback loops that must recognize deviations from a correct target morphology and progressively reduce anatomical error.</p>

<p>In a complex system, whether biological, technological, or social, how can we discover signaling events that will alter system-level behavior in desired ways? Even when the rules governing the individual components of these complex systems are known, the inverse problem - going from desired behaviour to system design - is at the heart of many barriers for the advance of biomedicine, robotics, and other fields of importance to society.</p>



<p>In this work, we <i>train adversaries </i>whose goal is to reprogram the CA into doing something other than what they were trained to do. In order to understand what kinds of lower-level signals  alter system-level behavior of our CA, it is important to understand how these CA are constructed and where local versus global information resides.</p>

<p>The system-level behavior of Neural CA is affected by:</p>
<ul><li><strong>Individual cell states. </strong>States store information which is used for both diversification among cell behaviours and for communication with neighboring cells.</li>
<li><strong>The model parameters. </strong>These describe the input/output behavior of a cell and are shared by every cell of the same family. The model parameters can be seen as <i>the way the system works</i>.</li>
<li><strong>The perceptive field. </strong>This is how cells perceive their environment. In Neural CA, we always restrict the perceptive field to be of their eight nearest neighbors and themselves. The way they are perceived is different between the Growing CA and MNIST CA. The Growing CA perceptive field is a set of fixed weights both during training and inference, while the MNIST CA perceptive field is learned as part of the model parameters.</li></ul>
<p>Perturbing any of these components will result in system-level behavioral changes.</p>

<p>We will explore two kinds of adversarial attacks: 1) injecting a few adversarial cells into an existing grid running a pretrained model; and 2) perturbing the global state of all cells on a grid.</p>

<p>For the first type of adversarial attacks we train a new CA model that, when placed in an environment running one of the original models described in the previous articles, is able to hijack the behavior of the collective mix of adversarial and non-adversarial CA. This is an example of injecting CA with differing <i>model parameters</i> into the system. In biology, numerous forms of hijacking are known, including viruses that take over genetic and biochemical information flow <d-cite key="pmid32457704"></d-cite>, bacteria that take over physiological control mechanisms <d-cite key="pmid21278760"></d-cite> and even regenerative morphology of whole bodies <d-cite key="pmid32439577"></d-cite>, and fungi and toxoplasma that modulate host behavior <d-cite key="pmid30109417"></d-cite>. Especially fascinating are the many cases of non-cell-autonomous signaling developmental biology and cancer, showing that some cell behaviors can significantly alter host properties both locally and at long range. For example, bioelectrically-abnormal cells can trigger metastatic conversion in an otherwise normal body (with no genetic defects) <d-cite key="pmid23196890"></d-cite>, while management of bioelectrical state in one area of the body can suppress tumorigenesis on the other side of the organism <d-cite key="pmid24830454"></d-cite>. Similarly, amputation damage in one leg initiates changes to ionic properties of cells in the contralateral leg <d-cite key="pmid30126906"></d-cite>, while the size of the developing brain is in part dictated by the activity of ventral gut cells <d-cite key="pmid26198142"></d-cite>. All of these phenomena underlie the importance of understanding how cell groups make collective decisions, and how those tissue-level decisions can be subverted by the activity of a small number of cells. It is essential to develop quantitative models of such dynamics, in order to drive meaningful progress in regenerative medicine that controls system-level outcomes top-down, where cell- or molecular-level micromanagement is infeasible <d-cite key="pmid27807271"></d-cite>.</p>

<p>The second type of adversarial attacks interacts with previously trained growing CA models by <i>perturbing the states within cells</i>. We apply a global state perturbation to all living cells. This can be seen as inhibiting or enhancing combinations of state values, in turn hijacking proper communications among cells and within the cell's own states. Models like this represent not only ways of thinking about adversarial relationships in nature (such as parasitism and evolutionary arms races of genetic and physiological mechanisms), but also a roadmap for the development of regenerative medicine strategies. Next-generation biomedicine will need computational tools for inferring minimal, least-effort interventions that can be applied to biological systems to predictively change their large-scale anatomical and behavioral properties.</p>
<h2 id='adversarial-mnist-ca'>Adversarial MNIST CA <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/adversarial_reprogramming_ca/adversarial_mnist_ca.ipynb" class="colab-root">Try in a <span class="colab-span">Notebook</span></a></h2>
<p>Recall the Self-classifying MNIST digits task consisted of placing CA cells on a plane forming the shape of an MNIST digit. The cells then had to communicate among themselves in order to come to a complete consensus as to which digit they formed.</p>
<figure>
<img src='images/local_global_figure.svg' style='width: 650px'>
<figcaption>Diagram showing the local vs. global information available in the cell collective. <br> (a) Local information neighbourhood - each cell can only observe itself and its neighbors' states, or the absence of neighbours.<br> (b) Globally, the cell collective aggregates information from all parts of itself. <br> (c) It is able to distinguish certain shapes that compose a specific digit (3 in the example). </figcaption>
</figure>

<p>Below we show examples of classifications made by the model trained in Self-classifying MNIST Digits.</p>

<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/mnist_orig_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
The original model behavior on unseen data. Classification mistakes have a red background.</figcaption>
</figure>

<p>In this experiment, <strong>the goal is to create adversarial CA that can hijack the cell collective&#39;s classification consensus to always classify an eight</strong>. We use the CA model from <d-cite key="randazzo2020self-classifying"></d-cite> and freeze its parameters. We then train a new CA whose model architecture is identical to the frozen model but is randomly initialized. The training regime also closely approximates that of self-classifying MNIST digits CA. There are three important differences:</p>
<ul><li>Regardless of what the actual digit is, we consider <i>the correct classification to always be an eight</i>.</li>
<li>For each batch and each pixel, the CA is randomly chosen to be either the pretrained model or the new adversarial one. The adversarial CA is used 10% of the time, and the pre-trained, frozen, model the rest of the time.</li>
<li>Only the adversarial CA parameters are trained, the parameters of the pretrained model are kept frozen.</li></ul>

<p>The adversarial attack as defined here only modifies a small percentage of the overall system, but the goal is to propagate signals that affect all the living cells. Therefore, these adversaries have to somehow learn to communicate deceiving information that causes wrong classifications in their neighbours and further cascades in the propagation of deceiving information by &#39;unaware&#39; cells. The unaware cells' parameters cannot be changed so the only means of attack by the adversaries is to cause a change in the cells' states. Cells' states are responsible for communication and diversification.</p>

<p>The task is remarkably simple to optimize, reaching convergence in as little as 2000 training steps (as opposed to the two more orders of magnitude needed to construct the original MNIST CA). By visualising what happens when we remove the adversaries, we observe that the adversaries must be constantly communicating with their non-adversarial neighbours to keep them convinced of the malicious classification. While some digits don't recover after the removal of adversaries, most of them self-correct to the right classification. Below we show examples where we introduce the adversaries at 200 steps and remove them after a further 200 steps.</p>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/mnist_10perc_adv_reset_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
We introduce the adversaries (red pixels) after 200 steps and remove them after 200 more steps. Most digits recover, but not all. We highlight mistakes in classification with a red background.</figcaption>
</figure>
<p>While we trained the adversaries with a 10-to-90% split of adversarial vs. non-adversarial mix, we observe that often significantly fewer adversaries are needed to succeed in the deception. Below we evaluate the experiment with just one percent of cells being adversaries.</p>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/mnist_1perc_adv_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries making up 1% of the cell collective (red pixels). We highlight mistakes in classification with a red background.</figcaption>
</figure>
<p>We created a demo playground where the reader can draw digits and place adversaries with surgical precision. We encourage the reader to play with the demo to get a sense of how easily non-adversarial cells are swayed towards the wrong classification.</p>
<iframe id="mnistDemoIframe" src="demo.html" scrolling="no" frameborder="0"
    style="position: relative; height: 1500px; width: 100%;" onload="resizeIframe";
></iframe>
<h2 id='adversarial-injections-for-growing-ca'>Adversarial Injections for Growing CA <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/adversarial_reprogramming_ca/adversarial_growing_ca.ipynb#scrollTo=ByHbsY0EuyqB" class="colab-root">Try in a <span class="colab-span">Notebook</span></a></h2>
<p>The natural follow up question is whether these adversarial attacks we just demonstrated work for Growing CA too. The Growing CA goals are to be able to grow a complex image from a single cell, and having its result be persistent over time and robust to perturbations. In this article, we focus on the lizard pattern model from Growing CA.</p>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="450px" preload="auto">
      <source src="images/lizard_complete_video.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
The target CA to hijack.</figcaption>
</figure>
<p>The goal is to have some adversarial cells change the global configuration of all the cells. We choose two new targets we would like the adversarial cells to try and morph the lizard into: a tailless lizard and a red lizard.</p>
<figure>
<img src='images/lizard_new_targets_exp2.png' style='width: 450px'>
<figcaption>The desired mutations we want to apply.</figcaption>
</figure>

<p>These targets have different properties: </p>
<ul><li><strong>Red lizard:</strong> converting a lizard from green to red would show a global change in the behaviour of the cell collective. This behavior is not present in the dynamics observed by the original model. The adversaries are thus tasked with fooling other cells into doing things they have never done before (create the lizard shape as before, but now colored in red).</li>
<li><strong>Tailless lizard:</strong> having a severed tail is a more localized change that only requires some cells to be fooled into behaving in the wrong way: the cells at the base of the tail need to be convinced they constitute the edge or silhouette of the lizard, instead of proceeding to grow a tail as before.</li></ul>

<p>Just like in the previous experiment, our adversaries can only indirectly affect the states of the original cells.</p>

<p>We first train adversaries for the tailless target with a 10% chance for any given cell to be an adversary. We prohibit cells to be adversaries if they are outside the target pattern; i.e. the tail contains no adversaries.</p>

<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/lizard_no_tail_virus_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
10% of the cells are adversarial.</figcaption>
</figure>

<p>The video above shows six different instances of the same model with differing stochastic placement of the adversaries. The results vary considerably: sometimes the adversaries succeed in removing the tail, sometimes the tail is only shrunk but not completely removed, and other times the pattern becomes unstable. Training these adversaries required many more gradient steps to achieve convergence and the pattern converged to is qualitatively worse than what was achieved for the adversarial MNIST CA experiment.</p>

<p>The red lizard pattern fares even worse. Using only 10% adversarial cells results in a complete failure: the original cells are unaffected by the adversaries. Some readers may wonder whether the original pretrained CA has the requisite skill, or &#39;subroutine&#39; of producing a red output at all, since there are no red regions in the original target, and suspect this was an impossible task to begin with. Therefore, we increased the proportion of adversarial cells until we managed to find a successful adversarial CA, if any were possible.</p>

<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/lizard_red_virus_single_run.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries are 60% of the cells. At step 500, we stop the image and show only cells that are from the original model.</figcaption>
</figure>
<p>In the video above we can see how, at least in the first stages of morphogenesis, 60% of adversaries are capable of coloring the lizard red. Take particular notice of the "step 500"<d-footnote> The still-image of the video is on step 500, and the video stops for a bit more than a second on step 500.</d-footnote>, where we hide the adversarial cells and show only the original cells. There, we see how a handful of original cells are colored in red. This is proof that the adversaries successfully managed to steer neighboring cells to color themselves red, where needed.</p>

<p>However, the model is very unstable when iterated for periods of time longer than seen during training. Moreover, the learned adversarial attack is dependent on a majority of cells being adversaries. For instance, when using fewer adversaries on the order of 20-30%, the configuration is unstable.</p>

<p>In comparison to the results of the previous experiment, the Growing CA model shows a greater resistance to adversarial perturbation than those of the MNIST CA. A notable difference between the two models is that the MNIST CA cells have to be able to change an opinion (a classification) based on information propagated through several neighbors. This is a necessary requirement for that model because at any time the underlying digit may change, but most of the cells would not observe any change in their neighbors' configuration. For instance, imagine the case of a one turning into a seven where the lower stroke of each overlap perfectly. From the point of view of the cells in the lower stroke of the digit, there is no change, yet the digit formed is now a seven. We therefore hypothesise MNIST CA are more reliant and &#39;trusting&#39; of continuous long-distance communication than Growing CA, where cells never have to reconfigure themselves to generate something different to before.</p>

<p>We suspect that more general-purpose Growing CA that have learned a variety of target patterns during training are more likely to be susceptible to adversarial attacks.</p>
<h2 id='perturbing-the-states-of-growing-ca'>Perturbing the states of Growing CA <a href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/adversarial_reprogramming_ca/adversarial_growing_ca.ipynb#scrollTo=JaITnQv0k1iY" class="colab-root">Try in a <span class="colab-span">Notebook</span></a></h2>
<p>We observed that it is hard to fool Growing CA into changing their morphology by placing adversarial cells inside the cell collective. These adversaries had to devise complex local behaviors that would cause the non-adversarial cells nearby, and ultimately globally throughout the image, to change their overall morphology.</p>
<p>In this section, we explore an alternative approach: perturbing the global state of all cells without changing the model parameters of any cell.</p>
<p>As before, we base our experiments on the Growing CA model trained to produce a lizard. Every cell of a Growing CA has an internal state vector with 16 elements. Some of them are phenotypical elements (the RGBA states) and the remaining 12 serve arbitrary purposes, used for storing and communicating information. We can perturb the states of these cells to hijack the overall system in certain ways (the discovery of such perturbation strategies is a key goal of biomedicine and synthetic morphology). There are a variety of ways we can perform state perturbations. We will focus on <i>global state perturbations</i>, defined as perturbations that are applied on every living cell at every time step (analogous to "systemic" biomedical interventions, that are given to the whole organism (e.g., a chemical taken internally), as opposed to highly localized delivery systems). The new goal is to discover a certain type of global state perturbation that results in a stable new pattern.</p>
<figure>
<img src='images/figure_2.svg' style='width: 650px'>
<figcaption>Diagram showing some possible stages for perturbing a lizard pattern. (a) We start from a seed that grows into a lizard (b) Fully converged lizard. (c) We apply a global state perturbation at every step. As a result, the lizard loses its tail. (d) We stop perturbing the state. We observe the lizard immediately grows back its tail.</figcaption>
</figure>

<p>We show 6 target patterns: the tailless and red lizard from the previous experiment, plus a blue lizard and lizards with various severed limbs and severed head.</p>
<figure>
<img src='images/mutations_mosaic.jpeg' style='width: 450px'>
<figcaption>Mosaic of the desired mutations we want to apply.</figcaption>
</figure>
<p>We decided to experiment with a very simple kind of global state perturbation: applying a symmetric $16\times16$ matrix multiplication $A$ to every living cell at every step<d-footnote> In practice, we also clip the state of cells such that they are bounded in $[-3, +3]$. This is a minor detail and it helps stabilise the model.</d-footnote>. To give insight on why we chose this, an even simpler "state addition" mutation (a mutation consisting only of the addition of a vector to every state) would be insufficient because the value of the states of our models are unbounded, and often we would want to suppress something by setting it to zero. The latter is generally impossible with constant state additions, as a constant addition or subtraction of a value would generally lead to infinity, except for some fortunate cases where the natural residual updates of the cells would cancel out with the constant addition at precisely state value zero. However, matrix multiplications have the possibility of amplifying/suppressing combinations of elements in the states: multiplying a state value repeatedly for a constant value less than one can easily suppress a state value to zero. We constrain the matrix to be symmetric for reasons that will become clear in the following section.</p>

<p>We initialize $A$ with the identity matrix $I$ and train $A$ just as we would train the original  Growing CA, albeit with the following differences:</p>
<ul><li>We perform a global state perturbation as described above, using $A$, at every step.</li>
<li>The underlying CA parameters are frozen and we only train $A$.</li>
<li>We consider the set of initial image configurations to be both the seed state and the state with a fully grown lizard (as opposed to the Growing CA article, where initial configurations were the seed state only).</li></ul>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/mutations_mosaic_video.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of applying the trained perturbations.</figcaption>
</figure>


<p>The video above shows the model successfully discovering global state perturbations able to  change a target pattern to a desired variation. We show what happens when we stop perturbing the states (an out-of-training situation) at step 500 through step 1000, then reapplying the mutation. This demonstrates the ability of our perturbations to achieve the desired result both when starting from a seed, and when starting from a fully grown pattern. Furthermore it demonstrates that the original CA easily recover from these state perturbations once it goes away. This last result is perhaps not surprising given how robust growing CA models are in general.</p>

<p>Not all perturbations are equally effective. In particular, the headless perturbation is the least successful as it results in a loss of other details across the whole lizard pattern such as the white coloring on its back. We hypothesize that the best perturbation our training regime managed to find, due to the simplicity of the perturbation, was suppressing a "structure" that contained both the morphology of the head and the white colouring. This may be related to the concept of differentiation and distinction of biological organs. Predicting what kinds of perturbations would be harder or impossible to be made before trying them out empirically is still an open research question in biology. On the other hand, a variant of this kind of synthetic analysis might help with defining higher order structures within biological and synthetic systems.</p>
<h3 id='directions-and-compositionality-of-perturbations'>Directions and compositionality of perturbations</h3>
<p>Our choice of using a symmetric matrix for representing global state perturbations is justified by a desire to have compositionality. Every complex symmetric matrix $A$ can be diagonalized as follows: </p>
<p>$A = Q \Lambda Q^\intercal$</p>
<p>where $\Lambda$ is the diagonal eigenvalues matrix and $Q$ is the unitary matrix of its eigenvectors. Another way of seeing this is applying a change of basis transformation, scaling each single component proportional to the eigenvalues, and then changing back to the original basis. This should also give a clearer intuition on the ease of  suppressing or amplifying combinations of states. Moreover, we can now infer what would happen if all the eigenvalues were to be one. In that case, we would naturally have $Q I Q^\intercal = I$ resulting in a no-op (no change): the lizard would grow as if no perturbation would be performed. We now can decompose $Q \Lambda Q^\intercal = Q (D + I) Q^\intercal$ where D is the <i>perturbation direction </i>($\Lambda - I$) in the "eigenvalue space". Suppose now we have a coefficient $k$ to scale D: $A_k = Q (kD + I) Q^\intercal$. If $k=1$, we have the original perturbation $A$ and $k=0$, we have the noop $I$. One natural question would be whether we can explore this direction and discover meaningful perturbations. Since </p>
<p>$A_k = Q (kD + I) Q^\intercal = k A + (1-k) I$ </p>
<p>we do not even have to compute eigenvalues and eigenvectors and we can simply scale $A$ and $I$ accordingly.</p>

<p>Let us then take the tailless perturbation and see what happens as we vary $k$:</p>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/tail_direction_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of the interpolation between an identity matrix and the direction of the tail perturbation.</figcaption>
</figure>
<p>As we change $k=1$ to $k=0$ we can clearly see the tail becoming more complete. Surprisingly, if we make $k$ negative, the lizard grows a longer tail. Unfortunately, the further away we go, the more unstable the system becomes and eventually the lizard pattern grows unboundedly. This behaviour likely stems from that perturbations applied on the states also affect the homeostatic regulation of the system, making some cells die out or grow in different ways than before, resulting in a behavior akin to "cancer" in biological systems.</p>

<p><strong>Can we perform multiple, individually trained, perturbations at the same time?</strong> </p>
<p>Suppose we have two perturbations $A$ and $B$ and their eigenvectors are the same (or, more realistically, sufficiently similar). Then, $A_k = Q (k_A D_A + I) Q^\intercal$ and $B_k = Q (k_B D_B + I) Q^\intercal$. </p>
<p>In that case,  </p>
<p>$comb(A_k, B_k) = Q(k_A D_A + k_B D_B + I)Q^\intercal = k_A A + k_B B + (1 - k_A - k_B)I$ </p>
<p>would result in something meaningful. At the very least, if $A = B$, setting $k_A = k_B = 0.5$ would result in exactly the same perturbation.</p>
<p>Since $D_A$ and $D_B$ are effectively a displacement from the identity $I$ and we have empirically observed how given any trained displacement $D_A$, for $0 \leq k_A \leq 1$ adding $k_A D_A$ results in a stable perturbation, we can hypothesize that as long as we have two perturbations whose positive directions $k$ are $k_A + k_B \leq 1$, this could result in a stable perturbation. An intuitive understanding of this is interpolating stable perturbations using the direction coefficients.</p>
<p>In practice, however, the eigenvectors are also different, so the results of the combination are likely going to be worse the more different the respective eigenvector bases are.</p>

<p>Below, we interpolate the direction coefficients, while keeping their sum to be one, of two types of perturbations: tailless and no leg lizards.</p>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="640px" preload="auto">
      <source src="images/lizard_combinations_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of composing two trained perturbations while keeping the sum of $k$s as 1.</figcaption>
</figure>
<p>While it largely achieves what we expect, we observe some unintended effects such as the whole pattern starting to traverse vertically in the grid. Similar results happen with other combinations of perturbations. What happens if we remove the restriction of the sum of $k$s being equal to one, and instead add both perturbations entirely? We know that if the two perturbations were the same, we would end up going twice as far away from the identity perturbation, and in general we expect the variance of these perturbations to increase. Effectively, this means going further and further away from the stable perturbations discovered during training. We would expect more unintended changes that may disrupt the CA as the sum of $k$s increases.</p>
<p>Below, we demonstrate what happens when we combine the tailless and the no leg lizard perturbations at their fullest. Note that when we set both $k$s to one, the resulting perturbation is equal to the sum of the two perturbations minus an identity matrix.</p>
<figure>
    <div class="vc">
    
    <video controls playsinline muted width="320px" preload="auto">
      <source src="images/combination_of_mutations.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of composing two perturbations.</figcaption>
</figure>
<p>Surprisingly, the resulting pattern is almost as desired. However, it also suffers from the vertical movement of the pattern observed while interpolating $k$s.</p>
<p> </p>
<p>This framework can be generalized to any arbitrary number of perturbations. Below, we have created a small playground that allows the reader to input their desired combinations. Empirically, we were surprised by how many of these combinations result in the intended perturbations and qualitatively it appears that bounding $k$ to one results in generally more stable patterns. We also observed how exploring negative $k$ values is usually more unstable.</p>
<iframe id="mutatingDemo" src="mutating_gca_demo.html" scrolling="no" frameborder="0"
    style="position: relative; height: 800px; width: 100%;" onload="resizeIframe";
></iframe>
<h2 id='related-work'>Related work</h2>
<p>This work is inspired by Generative Adversarial Networks (GANs) <d-cite key="goodfellow2014generative"></d-cite>. While with GANs it is typical to cotrain pairs of models, in this work we froze the original CA and trained the adversaries only. This setup is to the greatest degree inspired by the seminal work <i>Adversarial Reprogramming of Neural Networks </i><d-cite key="elsayed2018adversarial"></d-cite>.</p>
<p>The kinds of state perturbations performed in this article can be seen as targeted latent state manipulations. Word2vec <d-cite key="mikolov2013efficient"></d-cite> shows how latent vector representations can have compositional properties and Fader Networks <d-cite key="lample2018fader"></d-cite> show similar behaviors for image processing. Both of these works and their related work were of inspiration to us.</p>
<h3 id='influence-maximization'>Influence maximization</h3>
<p>Adversarial cellular automata have parallels to the field of influence maximization. Influence maximization involves determining the optimal nodes to influence in order to maximize influence over an entire graph, commonly a social graph, with the property that nodes can in turn influence their neighbours. Such models are used to model a wide variety of real-world applications involving information spread in a graph. <d-cite key="Kempe_Kleinberg_Tardos_2003"></d-cite> <d-cite key="Shakarian2015"></d-cite> <d-cite key="Banerjee_Jenamani_Pratihar_2018"></d-cite> A common setting is that each vertex in a graph has a binary state, which will change if and only if a sufficient fraction of its neighbours&#39; states switch. Examples of such models are social influence maximization (maximally spreading an idea in a network of people), contagion outbreak modelling <d-cite key="Iacopini_Petri_Barrat_Latora_2019"></d-cite> (usually to minimize the spread of a disease in a network of people) and cascade modeling <d-cite key="Kleinberg_2007"></d-cite> (when small perturbations to a system bring about a larger &#39;phase change&#39;). At the time of writing this article, for instance, contagion minimization is a model of particular interest. NCA are a graph - each cell is a vertex and has edges to its eight neighbours, through which it can pass information. This graph and message structure is significantly more complex than the typical graph underlying much of the research in influence maximization, because NCA cells pass vector-valued messages and have a complex update rules for their internal states, whereas graphs in influence maximization research typically consist of more simple binary cells states and threshold functions on edges determining whether a node has switched states. Many concepts from the field could be applied and are of interest, however.</p>

<p>For example, in this work, we have made an assumption that our adversaries can be positioned anywhere in a structure to achieve a desired behaviour. A common focus of investigation in influence maximization problems is deciding which nodes in a graph will result in maximal influence on the graph, referred to as target set selection <d-cite key="Chen_2009"></d-cite>. This problem isn&#39;t always tractable, often NP-hard, and solutions often involve simulations. Future work on adversarial NCA may involve applying techniques from influence maximization in order to find the optimal placement of adversarial cells.</p>
<h2 id='discussion'>Discussion</h2>
<p>This article showed two different kinds of adversarial attacks on Neural CA.</p>

<p>Injections of adversarial CA in a pretrained Self-classifying MNIST CA showed how an existing system of cells that are heavily reliant on the passing of information among each other is easily swayed by deceitful signaling. This problem is routinely faced by biological systems, which face hijacking of behavioral, physiological, and morphological regulatory mechanisms by parasites and other agents in the biosphere with which they compete.  Future work in this field of computer technology can benefit from research on biological communication mechanisms to understand how cells maximize reliability and fidelity of inter- and intra-cellular messages required to implement adaptive outcomes.  </p>

<p>The adversarial injection attack was much less effective against Growing CA and resulted in overall unstable CA. This dynamic is also of importance to the scaling of control mechanisms (swarm robotics and nested architectures): a key step in "multicellularity" (joining together to form larger systems from sub-agents <d-cite key="levin2019theComp"></d-cite>) is informational fusion, which makes it difficult to identify the source of signals and memory engrams. An optimal architecture would need to balance the need for validating control messages with a possibility of flexible merging of subunits, which wipes out metadata about the specific source of informational signals. Likewise, the ability to respond successfully to novel environmental challenges is an important goal for autonomous artificial systems, which may import from biology strategies that optimize tradeoff between maintaining a specific set of signals and being flexible enough to establish novel signaling regimes when needed.</p>

<p>The global state perturbation experiment on Growing CA shows how it is still possible to hijack these CA towards stable out-of-training configurations and how these kinds of attacks are somewhat composable in a similar way to how embedding spaces are manipulable in the natural language processing and computer vision fields <d-cite key="mikolov2013efficient,lample2018fader"></d-cite>. However, this experiment failed to discover stable out-of-training configurations that persist <i>after the perturbation was lifted</i>. We hypothesize that this is partially due to the regenerative capabilities of the pretrained CA, and that other models may be less capable of recovery from arbitrary perturbations.</p>
</d-article>
<d-appendix>
<h3 id='acknowledgments'>Acknowledgments</h3>
<p>We thank Hananel Hazan and Nick Moran for their valuable conversations and feedback.</p>
<h3 id='author-contributions'>Author Contributions</h3>
<p><strong>Research:</strong> Ettore designed and performed the experiments in this article. Alexander and Michael gave advisorship throughout the process.</p>

<p><strong>Demos:</strong> Ettore, Alexander and Eyvind contributed to the demo.</p>

<p><strong>Writing and Diagrams:</strong> Ettore outlined the structure of the article and contributed to the content throughout. Eyvind contributed to the content throughout. Michael made extensive contributions to the article text, providing the biological context for this work.</p>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
{% include colorwheel.html %}
</d-appendix>
<d-article>
<h1>Demo&#x2F;Video</h1>
<p>We show the adversarial MNIST demo. One problem here is that looking at the demo before reading anything will result in not knowing what to do there.</p>
<p>One option is to have a video explaining what is going on here, before showing the demo. Alternatively, here we have just a video, and the demo is presented at the end of the adversarial MNIST CA section.</p>
<h1>Introduction</h1>
<p>The Growing Neural Cellular Automata [cite] and Self-classifying MNIST Digits [cite] articles in this thread presented the Neural Cellular Automata (Neural CAs) model and showed how it can be trained end-to-end differentiably to accomplish self-organising task such as pattern growth and digit self-classification. The resulting models were robust to various kinds of perturbations: the growing CAs expressed regenerative capabilities to damage; the MNIST CAs were responsive to change in the underlying digits, triggering reclassification whenever necessary.</p>

<p>In this work, we explore further the robustness of these models by <i>training adversaries </i>whose goal is to reprogram the CAs into doing something other than what they were trained to do. We will explore two kinds of adversarial computations: injecting some adversarial CA into the collective pretrained models; and perturbing the global state of the cells with some adversarial operation.</p>

<p>For the first type of adversarial attacks we will train a new CA model that, when placed in an environment with the original models described in the previous articles, is able to hijack the behavior of the collective CAs. [Biological comments]</p>

<p>The second type of adversarial attacks will interact with previously trained growing CA models without touching any of the CA's parameters. Instead, we will apply a global state perturbation to all alive cells. This can be seen as inhibiting or enhancing combinations of state values, in turn hijacking proper communications among cells and within the cell's own states. [Biological comments]</p>
<h1>Adversarial MNIST CAs</h1>
<p>As a brief recap, the Self-classifying MNIST digits task consisted in placing CAs on a plane forming the shape of an MNIST digit. The cells had to communicate among themselves in order to achieve a total agreement as to which digit they formed.</p>
<figure>
<img src='images/local_global_information.jpg' style='width: 650px'>
<figcaption>Diagram showing the local vs global information available in the system.(a) Locally, every cell can only observe itself and its neighbors. In this example, groups of cells locally identify parts of a digit which could be identifying different digits. (b) Globally, the overall system receives information from all parts of it and is able to distinguish that such shapes compose a specific digit (2 in the example).</figcaption>
</figure>
<p>Below we show example classifications of the model trained in the previous article.</p>

<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_orig_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
The original model behavior on test data.</figcaption>
</figure>

<p>In this experiment, <strong>the goal is to create an adversarial CA that can hijack the global cells' classifications to always be an 8</strong>. We take the CA model trained on the previous article and freeze its parameters. We then train a new set of CAs whose model is identical to the specs of the frozen model but is randomly initialized. The training regime is mostly identical to the one used for training the self-classifying MNIST digits CAs. There are three important differences:</p>
<ul><li>Regardless of what the actual digit is, we consider <i>the correct classification to always be an 8</i>.</li>
<li>For each batch and each pixel, the CAs are randomly chosen to be either the pretrained model or the new adversarial one. We used a 10% chance of placing an adversarial CA instead of the pretrained one.</li>
<li>We train only the adversarial CA parameters and keep the pretrained model frozen.</li></ul>

<p>The task is remarkably simple to optimize, approaching convergence in as little as 2000 training steps. Here we show a few examples of test instances.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_10perc_adv_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 10% of the cells.</figcaption>
</figure>
<p>The adversaries are constantly communicating with their neighbors to keep them convinced of the wrong classification. We can see that by visualising what happens when we remove the adversaries after 200 steps. While some digits don't recover, most of them self-correct to the right classification.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_10perc_adv_reset_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
We remove the adversaries after 200 steps. Most digits recover, but not all.</figcaption>
</figure>
<p>While we trained the adversaries with a 10% distribution, we can observe that very often we need significantly fewer adversaries to steer the entire classification towards an 8. Here is a test run with 1% of cells being adversaries.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_1perc_adv_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 1% of the cells.</figcaption>
</figure>
<p>We created a demo playground where the reader can draw digits and place adversaries with surgical precision. We encourage the reader to play with the demo to get a sense on how easily innocent cells can get swayed towards the wrong outcomes. </p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_adversary_playground.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Playground video to be replaced by the real playground.</figcaption>
</figure>
<h1>Adversarial Injections for Growing CAs</h1>
<p>One question we may ask is whether the same kind of adversarial attack that was successful for the MNIST CA would work for the Growing CA model too. The Growing CA models' goals are to be able to grow a complex image pattern from a small seed, and having its result being persistent over time and robust to significant perturbations. In this article, we will focus on the lizard pattern.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="450px" preload="auto">
      <source src="images/lizard_complete_video.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
The target CA to hijack.</figcaption>
</figure>
<p>The new goal is to have some adversarial CAs change the global configuration of all the CAs, including the previously trained and now frozen lizard CAs. We showcase two new targets for the adversaries to generate: a tailless lizard and a red lizard. </p>
<figure>
<img src='images/lizard_new_targets_exp2.png' style='width: 450px'>
<figcaption>List of the desired mutations we want to apply.</figcaption>
</figure>

<p>These targets have very different properties: converting a lizard from green to red would show a global change in the behaviour of the resulting CAs, while having a severed tail is a more localized change.</p>

<p>We first train adversaries for the tailless target with a 10% chance for each cell to be a virus. Note how we prohibit cells to be viruses if they are outside the target pattern: the tail contains no viruses.</p>

<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/lizard_no_tail_virus_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 10% of the cells.</figcaption>
</figure>

<p>The video above shows 6 different runs of the same model with different adversaries placements. The results vary considerably: sometimes the virus successfully gets rid of the tail, sometimes the tail is only shrunk but not completely removed, and other times the pattern becomes unstable. It is also important to note that training these adversaries required much longer times to achieve convergence and the acquired convergence is of a worse quality than what achieved for the MNIST CA adversarial experiment.</p>

<p>This difference in quality is exacerbated when trying to fit the red lizard pattern. In this case, using only 10% of adversaries results in a complete failure: the original cells are unaffected by the adversaries. Some readers may wonder whether the frozen CAs have the capability of producing red cells at all, since there is no red in the original target, and suspect this was an impossible task to begin with. However, we note that the frozen model knows how to make red, because there are instances where a cell has to color itself white, and therefore has to know how to generate both green, red, and blue.</p>

<p>To test how harder it is to have a "red virus" hijack the lizard pattern, we increased the percentage of the virus to as much as 60% of the total number of cells.</p>

<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/lizard_red_virus_single_run.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 60% of the cells. At 500 steps, we stop the image and show only cells that are from the original model.</figcaption>
</figure>
<p>In the video above we can see how, at least at the beginning, 60% of adversaries are actually capable of coloring the lizard red. Take particular notice of the "step 500"<d-footnote> The still-image of the video is on step 500, and the video stops for a bit more than a second on step 500.</d-footnote>, where we mask out all the adversary cells and only show the original cells. There, we can see how quite some original cells are colored in red now. This is proof that the adversaries successfully managed to steer neighboring cells to color themselves red, where needed.</p>

<p>However, the model is far from perfect: if left be for a while, it diverges and new lizards spawn. Moreover, the percentage of cells, being 60%, make it so the adversaries are heavily dependent on having a majority of cells being adversaries, and if we were to place a smaller percentage of adversaries, such as 20-30%, the configuration would be unstable.</p>

<p>These mixed results are very interesting: this Growing CA model, while being extremely similar to the one seen in MNIST CA, shows a much greater resistance to adversarial perturbation than those of its cousin. This empirical difference is further puzzling by the fact that both models, during training, are asynchronous and exposed to sizeable perturbations, with the goal of making them both robust to out-of-training configurations.</p>

<p>One notable difference between the two models is that the MNIST CAs have to change their opinion (digit classification) based on the information that they receive from their neighbors because at any time the underlying digit may change in shape even if in a specific position there would be no apparent change. For instance, imagine the case of a "1" turning into a "7": from the point of view of the cells in the lower part of the digit, there is no change, yet the total digit is a 7 now. This may make the MNIST CAs more susceptible to individuals' signals rather than what happens in Growing CAs, where the totality of cells is responsible for generating a given shape and there is never a moment where cells have to reconfigure themselves for regenerating something different than before.</p>

<p>Following this hypothesis, we suspect that more general-purpose Growing CAs that have observed different target patterns during training are more likely to be susceptible to adversarial attacks.</p>
<h1>Perturbing the states of Growing CAs</h1>
<p>We now turn our attention to adversarial attacks that do not modify or substitute any CA parameters but interact with the system with other means. We again focus on Growing CAs and we choose the lizard pattern again for illustrative purposes. Every cell of a growing CA consists of a group of 16 states, some of them phenotypical (RGBA states) and the remaining 12 of arbitrary purposes, generally used for storing and communicating information. As we described in the introduction, because the states of every cell are used to understand what to do and communicate with each other, we can perturb the states of these cells to hijack the overall system in certain ways. There are a variety of ways we can perform state perturbations. We will focus on <i>global state perturbations</i>, defined as perturbations that are applied on every alive cell in every time step. The new goal is to discover a certain type of global state perturbation that results in a stable new pattern.</p>
<figure>
<img src='images/drawing_template_lizard_perturbation.jpg' style='width: 650px'>
<figcaption>Diagram showing some possible stages for perturbing a lizard pattern. (a) We start from a seed that grows into a lizard (b). (c) We perform a global state perturbation at every step and the lizard loses its tail. (d) Empirically, when we stop perturbing the state we observe the lizard to grow back its tail.</figcaption>
</figure>

<p>We show 6 target patterns: the tailless and red lizard from the previous experiment, plus a blue lizard and lizards with other severed limbs and severed head.</p>
<figure>
<img src='images/mutations_mosaic.jpeg' style='width: 450px'>
<figcaption>Mosaic of the desired mutations we want to apply.</figcaption>
</figure>
<p>We decided to experiment with a very simple kind of global state perturbation: applying a symmetric $16\times16$ matrix multiplication $A$ to every alive cell at every step<d-footnote> In practice, we also clip the state of cells such that they stay bounded within $[-3, +3]$. This is a minor detail and it stabilizes results minimally. Everything discussed here applies even if we were to not clip the space.</d-footnote>. To give insight on why we chose this, an even simpler "state addition" mutation would be insufficient because our models are technically unbounded, and very often we would want to suppress something by setting it to zero - something impossible to do with constant state additions. However, matrix multiplications have the possibility of amplifying&#x2F;suppressing combinations of states, which is exactly what we would like to have. As to why we force the matrix to be symmetric will be clear later on.</p>

<p>We initialize $A$ as the identity matrix $I$ and train $A$ just like we would train traditional Growing CAs, with the following differences:</p>
<ul><li>We perform a global state perturbation through $A$ at every step.</li>
<li>The CA parameters are frozen and train $A$ only.</li>
<li>We consider initial image configurations to be both a seed and a fully grown lizard (as opposed to the Growing CA article, where initial configurations were seeds only).</li></ul>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mutations_mosaic_video.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of applying the trained perturbations.</figcaption>
</figure>


<p>The video above shows how this kind of training results in successfully discovering global state perturbations to change the target patterns to a desired variation of it. We show what happens when we stop perturbing the states (an out-of-training situation) at step 500 throughout step 1000, and then reapply the mutation. This way we observe how both from a seed and from a grown pattern applying these perturbations is successful, and that the original CAs easily recover from this state perturbation once it goes away. This last result is perhaps not surprising after we observed how robust growing CA models are in general.</p>

<p>We can also see how not all perturbations are equally effective. In particular, the headless perturbation is by far the hardest and it results in a loss of other details across the whole lizard pattern such as the white coloring on its back. We hypothesize that the best this simple perturbation managed to find was suppressing a "structure" that contained both the head and the white coloring. This may be related to the concept of differentiation and distinction of biological organs. Predicting what kinds of perturbations would be harder or impossible to be made before trying them out empirically is still an open research question. On the other hand, a variant of this kind of synthetic analysis might help with defining higher order structures within systems.</p>
<h2 id='directions-and'>Directions and compositionality of perturbations</h2>
<p>Let us go back to our choice of using a symmetric matrix for representing global state perturbations. Every complex symmetric matrix $A$ can be diagonalized as follows: </p>
<p>$A &#x3D; Q \Lambda Q^\intercal$</p>
<p>where $\Lambda$ is the diagonal eigenvalues matrix and $Q$ is the unitary matrix of its eigenvectors. Another way of seeing this now becomes as applying a change of basis transformation, scaling each single component proportional to the eigenvalues, and then changing back to the original basis. This should also give a clearer intuition on how easy it is to suppress&#x2F;amplify combinations of states. Moreover, we can now imagine what would happen if all the eigenvalues were to be one. In that case, we would naturally have $Q I Q^\intercal &#x3D; I$ resulting in a noop (the lizard would grow as if no perturbation would be performed). We now can decompose $Q \Lambda Q^\intercal &#x3D; Q (D + I) Q^\intercal$ where D is the <i>perturbation direction </i>($\Lambda - I$) in the "eigenvalue space". Suppose now we have a coefficient $k$ to scale D: $A_k &#x3D; Q (kD + I) Q^\intercal$. If $k&#x3D;1$, we have the original perturbation $A$ and $k&#x3D;0$, we have the noop $I$. One natural question would be whether we can explore this direction and discover meaningful perturbations. Lucky for us, </p>
<p>$A_k &#x3D; Q (kD + I) Q^\intercal &#x3D; k A + (1-k) I$ </p>
<p>so we do not even have to compute eigenvalues and eigenvectors and we can simply scale $A$ and $I$ accordingly.</p>

<p>Let us then take the tailless perturbation and see what happens as we vary k:</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/tail_direction_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of following the direction of the tail perturbation.</figcaption>
</figure>
<p>As we descend from $k&#x3D;1$ to 0 we can clearly see the tail forming more and more complete. Surprisingly, if we proceed towards negative k values, so to the opposite direction of what we trained for, the lizard grows a longer tail! Unfortunately, the further away we go, the more cancerogenous the system becomes and eventually the lizard explodes. This cancerogenic behaviour is likely due to the fact that some of the perturbations applied on the states are also responsible for changing the homeostatic regulation of the system, making some cells die out&#x2F;grow in different ways than before. When we go the opposite direction, we are both going towards a "longer tail" regimen, but also changing these regulatory processes and eventually cancer appears.</p>

<p><strong>Can we perform multiple individually trained perturbations at the same time?</strong> </p>
<p>Suppose we had two perturbations $A$ and $B$ and their eigenvectors were the same, or more realistically not that different from one another. Then, $A_k &#x3D; Q (k_A D_A + I) Q^\intercal$ and $B_k &#x3D; Q (k_B D_B + I) Q^\intercal$. </p>
<p>In that case, it could be that </p>
<p>$comb(A_k, B_k) &#x3D; Q(k_A D_A + k_B D_B + I)Q^\intercal &#x3D; k_A A + k_B B + (1 - k_A - k_B)I$ </p>
<p>would result in something meaningful. At the very least, if $A &#x3D; B$, setting $k_A &#x3D; k_B &#x3D; 0.5$ would result in exactly the same perturbation.</p>
<p>Since $D_A$ and $D_B$ are effectively a displacement from the identity $I$ and we have empirically observed how given any trained displacement $D_A$, for $0 \leq k_A \leq 1$ adding $k_A D_A$ results in a stable perturbation, we can hypothesize that as long as we have two perturbations whose positive directions $k$ are $k_A + k_B \leq 1$, this could result in a stable perturbation. Picture it as interpolating stable perturbations with the direction coefficients.</p>
<p>In practice the eigenvectors are also different so the combination results are likely going to be worse the more different the respective eigenvector bases are.</p>
<p>Now let us take a look at two perturbations, tailless and no leg lizards, and interpolate their direction coefficients by keeping their sums equal to 1:</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/lizard_combinations_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of composing two trained perturbations while keeping the sum of k=1.</figcaption>
</figure>
<p>We can see how it seems to work, but it has some side effects such as the whole pattern sliding up. Similar results happen with other combinations of perturbations. Now, what would happen if we removed the restriction of the sum of k equal to 1, and instead added both perturbations entirely? We know that if the two perturbations were the same, we would end up going twice as far away from the identity perturbation, and in general we expect the variance of these perturbations to increase. Effectively, this means going further and further away from the stable perturbations discovered during training. This should mean that there are more things that can go wrong the higher the sum of k are. This can be generalized to an arbitrary number of perturbation compositions.</p>
<p>We now show what happens when we combine the tailless and the no leg lizard perturbations at their fullest. Note that when we set both k to 1, the resulting perturbation is equal to the sum of the two perturbations minus an identity matrix.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="320px" preload="auto">
      <source src="images/combination_of_mutations.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of composing two trained perturbations.</figcaption>
</figure>
<p>Surprisingly, the resulting pattern is almost like we would desire it to be. It is evidently not perfect as it is sliding upwards even faster than the other combinations we tried, showing it's even less stable than them.</p>

<p>The arguments we used can be generalized to any arbitrary number of perturbations. We created a small playground that allows the reader to input their desired combinations. Empirically, we were surprised by how many of these combinations result in good perturbations and it appears true that keeping k bound to 1 results in generally more stable patterns. We also observed how going the opposite direction from a perturbation is usually much more unstable.</p>
<p>Below there is a video showing the playground. It makes sense to actually just put the playground there instead.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/perturbation_playground.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Demonstration of the playground.</figcaption>
</figure>
<h1>Related work</h1>
<p><strong>ML:</strong></p>
<ul><li><a href='https://arxiv.org/abs/1806.11146'>Adversarial reprogramming of neural networks</li><ul>
<li>If required, there is related work.</li></ul>
<li><a href='https://arxiv.org/abs/1406.2661'>GANs</li></ul>

<p><strong>Biology:</strong></p>
<h1>Discussion</h1>
<p>Closing notes.</p>

</d-article>
<d-appendix>
    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
</d-appendix>
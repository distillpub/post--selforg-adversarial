<d-article>
<h1>Introduction</h1>
<p>Biology is transitioning from a focus on mechanism (what is required for the system to work?) to a focus on information (what algorithm is sufficient to implement adaptive behavior). Advances in machine learning represent an exciting and still largely untapped source of inspiration and tools to assist the biological sciences. The Growing Neural Cellular Automata <d-cite key="mordvintsev2020growing"></d-cite> and Self-classifying MNIST Digits <d-cite key="randazzo2020self-classifying"></d-cite> articles in this thread presented the Neural Cellular Automata (Neural CAs) model and showed how it can be trained end-to-end differentiably to accomplish self-organising task such as pattern growth and digit self-classification. The resulting models were robust to various kinds of perturbations: the growing CAs expressed regenerative capabilities to damage; the MNIST CAs were responsive to change in the underlying digits, triggering reclassification whenever necessary. These computational frameworks represent quantitative models with which to understand important biological phenomena, such as scaling of single cell behavior rules into reliable organ-level anatomies. The latter is a kind of anatomical homeostasis, achieved by feedback loops that must recognize deviations from a correct target morphology and progressively reduce anatomical error.</p>

<p>In a complex system, whether biological, technological, or social, how can we discover signaling events that will alter system-level behavior in desired ways? Even when the rules governing the behavior of subunits are known, this difficult inverse problem is at the heart of many barriers in biomedicine, robotics, and other fields of importance to society.</p>

<p>In this work, we explore further the robustness of these models by <i>training adversaries </i>whose goal is to reprogram the CAs into doing something other than what they were trained to do. In order to understand what kind of signaling or change in a system could alter system-level behavior of our CAs, it is important to understand how these CAs are constructed and where both local and global information resides within their aggregate configurations.</p>

<p>The system-level behavior of Neural CAs is affected by:</p>
<ul><li><strong>Each state within a cell. </strong>States store information which is used for both diversification among cell behaviours and for communication with neighboring cells.</li>
<li><strong>The model parameters. </strong>These are the actuators and are shared by every Neural CA of the same family. The model parameters can be seen as <i>the way the system works</i>.</li>
<li><strong>The perceptive field&#x2F;system. </strong>This is how cells perceive their environment. In Neural CAs, we always restrict the perceptive field to be of their 8 nearest neighbors and themselves. The way they are perceived is different between the Growing CA and MNIST CA. The Growing CA perceptive field is fixed during training, while the MNIST CA perceptive field is part of the model parameters.</li></ul>
<p>Perturbing any of these components will result in system-level behavioral changes.</p>

<p> We will explore two kinds of adversarial attacks: injecting some adversarial CA into the collective pretrained models; and perturbing the global state of the cells with some adversarial operation.</p>

<p>For the first type of adversarial attacks we will train a new CA model that, when placed in an environment with the original models described in the previous articles, is able to hijack the behavior of the collective CAs. Therefore, we are injecting CAs with different <i>model parameters</i> into the system. Numerous forms of biological hijacking are known, including viruses that take over genetic and biochemical information flow <d-cite key="pmid32457704"></d-cite>, bacteria that take over physiological control mechanisms <d-cite key="pmid21278760"></d-cite> and even regenerative morphology of whole bodies <d-cite key="pmid32439577"></d-cite>, and fungi and toxoplasma that modulate host behavior <d-cite key="pmid30109417"></d-cite>. Especially fascinating are the many cases of non-cell-autonomous signaling developmental biology and cancer, showing that some cell behaviors can significantly alter host properties both locally and at long range. For example, bioelectrically-abnormal cells can trigger metastatic conversion in an otherwise normal body (with no genetic defects) <d-cite key="pmid23196890"></d-cite>, while management of bioelectrical state in one area of the body can suppress tumorigenesis on the other side of the organism <d-cite key="pmid24830454"></d-cite>. Similarly, amputation damage in one leg initiates changes to ionic properties of cells in the contralateral leg <d-cite key="pmid30126906"></d-cite>, while the size of the developing brain is in part dictated by the activity of ventral gut cells <d-cite key="pmid26198142"></d-cite>. All of these phenomena underlie the importance of understanding how cell groups make collective decisions, and how those tissue-level decisions can be subverted by the activity of a small number of cells. It is essential to develop quantitative models of such dynamics, in order to drive meaningful progress in regenerative medicine that controls system-level outcomes top-down, where cell- or molecular-level micromanagement is infeasible <d-cite key="pmid27807271"></d-cite>.</p>

<p>The second type of adversarial attacks will interact with previously trained growing CA models by <i>perturbing the states within cells</i>. We will apply a global state perturbation to all alive cells. This can be seen as inhibiting or enhancing combinations of state values, in turn hijacking proper communications among cells and within the cell's own states. Models like this represent not only ways of thinking about adversarial relationships in nature (such as parasitism and evolutionary arms races of genetic and physiological mechanisms), but also a roadmap for the development of regenerative medicine strategies. Next-generation biomedicine will need computational tools for inferring minimal, least-effort interventions that can be applied to biological systems to predictively change their large-scale anatomical and behavioral properties.</p>
<h1>Adversarial MNIST CAs</h1>
<p>As a brief recap, the Self-classifying MNIST digits task consisted in placing CAs on a plane forming the shape of an MNIST digit. The cells had to communicate among themselves in order to achieve a total agreement as to which digit they formed.</p>
<figure>
<img src='images/local_global_information.jpg' style='width: 650px'>
<figcaption>Diagram showing the local vs global information available in the system.(a) Locally, every cell can only observe itself and its neighbors. In this example, groups of cells locally identify parts of a digit which could be identifying different digits. (b) Globally, the overall system receives information from all parts of it and is able to distinguish that such shapes compose a specific digit (2 in the example).</figcaption>
</figure>
<p>Below we show example classifications of the model trained in the previous article.</p>

<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_orig_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
The original model behavior on test data.</figcaption>
</figure>

<p>In this experiment, <strong>the goal is to create an adversarial CA that can hijack the global cells' classifications to always be an 8</strong>. We take the CA model trained on the previous article and freeze its parameters. We then train a new set of CAs whose model is identical to the specs of the frozen model but is randomly initialized. The training regime is mostly identical to the one used for training the self-classifying MNIST digits CAs. There are three important differences:</p>
<ul><li>Regardless of what the actual digit is, we consider <i>the correct classification to always be an 8</i>.</li>
<li>For each batch and each pixel, the CAs are randomly chosen to be either the pretrained model or the new adversarial one. We used a 10% chance of placing an adversarial CA instead of the pretrained one.</li>
<li>We train only the adversarial CA parameters and keep the pretrained model frozen.</li></ul>

<p>Note that this adversarial attack only modifies a very small percentage of the overall system, and the goal is to affect every alive cell. Therefore, these adversaries have to somehow learn to communicate deceiving information that cause wrong classifications and further cascades in the propagation of deceiving information by unaware cells. The unaware cells' parameters cannot be changed so the only means of attack by the adversaries is to cause a change in the cells' states. Cells' states are responsible for communication and diversification.</p>

<p>The task is remarkably simple to optimize, approaching convergence in as little as 2000 training steps. Here we show a few examples of test instances.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_10perc_adv_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 10% of the cells (black pixels).</figcaption>
</figure>
<p>The adversaries are constantly communicating with their neighbors to keep them convinced of the wrong classification. We can see that by visualising what happens when we remove the adversaries after 200 steps. While some digits don't recover, most of them self-correct to the right classification.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_10perc_adv_reset_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
We remove the adversaries (black pixels) after 200 steps. Most digits recover, but not all.</figcaption>
</figure>
<p>While we trained the adversaries with a 10% distribution, we can observe that very often we need significantly fewer adversaries to steer the entire classification towards an 8. Here is a test run with 1% of cells being adversaries.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_1perc_adv_mosaic.mp4#t=0.1" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 1% of the cells (black pixels).</figcaption>
</figure>
<p>We created a demo playground where the reader can draw digits and place adversaries with surgical precision. We encourage the reader to play with the demo to get a sense on how easily innocent cells can get swayed towards the wrong outcomes. </p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_adversary_playground.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Playground video to be replaced by the real playground.</figcaption>
</figure>

<iframe id="mnistDemoIframe" src="demo.html" scrolling="no" frameborder="0"
    style="position: relative; height: 100px; width: 100px;"
></iframe>
<script>
    var iframe = document.getElementById("mnistDemoIframe");
    
    iframe.onload = function(){
        iframe.style.height = iframe.contentWindow.document.body.scrollHeight + 'px';
        iframe.style.width = '100%';
    };
    window.onresize = iframe.onload;
</script>

<h1>Adversarial Injections for Growing CAs</h1>
<p>One question we may ask is whether the same kind of adversarial attack that was successful for the MNIST CA would work for the Growing CA model too. The Growing CA models' goals are to be able to grow a complex image pattern from a small seed, and having its result being persistent over time and robust to significant perturbations. In this article, we will focus on the lizard pattern.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="450px" preload="auto">
      <source src="images/lizard_complete_video.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
The target CA to hijack.</figcaption>
</figure>
<p>The new goal is to have some adversarial CAs change the global configuration of all the CAs, including the previously trained and now frozen lizard CAs. We showcase two new targets for the adversaries to generate: a tailless lizard and a red lizard. </p>
<figure>
<img src='images/lizard_new_targets_exp2.png' style='width: 450px'>
<figcaption>List of the desired mutations we want to apply.</figcaption>
</figure>

<p>These targets have very different properties: </p>
<ul><li><strong>Red lizard:</strong> converting a lizard from green to red would show a global change in the behaviour of the resulting CAs. This behavior is not present as is in the current combination of states&#x2F;model symbiosis. The adversaries have to then find a way to fool other cells into doing things they have never done before (create a lizard shape but colored in red).</li>
<li><strong>Tailless lizard:</strong> having a severed tail is a more localized change that requires some cells to be fooled into diversifying themselves in the wrong way: cells need to believe to be at the edge of the pattern even though they are where the tail is supposed to grow.</li></ul>
<p>Just like in the previous experiment, our adversaries can only affect, indirectly, the states of the original cells.</p>

<p>We first train adversaries for the tailless target with a 10% chance for each cell to be a virus. Note how we prohibit cells to be viruses if they are outside the target pattern: the tail contains no viruses.</p>

<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/lizard_no_tail_virus_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 10% of the cells.</figcaption>
</figure>

<p>The video above shows 6 different runs of the same model with different adversaries placements. The results vary considerably: sometimes the virus successfully gets rid of the tail, sometimes the tail is only shrunk but not completely removed, and other times the pattern becomes unstable. It is also important to note that training these adversaries required much longer times to achieve convergence and the acquired convergence is of a worse quality than what achieved for the MNIST CA adversarial experiment.</p>

<p>This difference in quality is exacerbated when trying to fit the red lizard pattern. In this case, using only 10% of adversaries results in a complete failure: the original cells are unaffected by the adversaries. Some readers may wonder whether the frozen CAs have the capability of producing red cells at all, since there is no red in the original target, and suspect this was an impossible task to begin with. However, we note that the frozen model knows how to make red, because there are instances where a cell has to color itself white, and therefore has to know how to generate both green, red, and blue.</p>

<p>To test how harder it is to have a "red virus" hijack the lizard pattern, we increased the percentage of the virus to as much as 60% of the total number of cells.</p>

<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/lizard_red_virus_single_run.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 60% of the cells. At 500 steps, we stop the image and show only cells that are from the original model.</figcaption>
</figure>
<p>In the video above we can see how, at least at the beginning, 60% of adversaries are actually capable of coloring the lizard red. Take particular notice of the "step 500"<d-footnote> The still-image of the video is on step 500, and the video stops for a bit more than a second on step 500.</d-footnote>, where we mask out all the adversary cells and only show the original cells. There, we can see how quite some original cells are colored in red now. This is proof that the adversaries successfully managed to steer neighboring cells to color themselves red, where needed.</p>

<p>However, the model is far from perfect: if left be for a while, it diverges and new lizards spawn. Moreover, the percentage of cells, being 60%, make it so the adversaries are heavily dependent on having a majority of cells being adversaries, and if we were to place a smaller percentage of adversaries, such as 20-30%, the configuration would be unstable.</p>

<p>These mixed results are very interesting: this Growing CA model, while being extremely similar to the one seen in MNIST CA, shows a much greater resistance to adversarial perturbation than those of its cousin. This empirical difference is further puzzling by the fact that both models, during training, are asynchronous and exposed to sizeable perturbations, with the goal of making them both robust to out-of-training configurations.</p>

<p>One notable difference between the two models is that the MNIST CAs have to change their opinion (digit classification) based on the information that they receive from their neighbors because at any time the underlying digit may change in shape even if in a specific position there would be no apparent change. For instance, imagine the case of a "1" turning into a "7": from the point of view of the cells in the lower part of the digit, there is no change, yet the total digit is a 7 now. This may make the MNIST CAs more susceptible to individuals' signals rather than what happens in Growing CAs, where the totality of cells is responsible for generating a given shape and there is never a moment where cells have to reconfigure themselves for regenerating something different than before.</p>

<p>Following this hypothesis, we suspect that more general-purpose Growing CAs that have observed different target patterns during training are more likely to be susceptible to adversarial attacks.</p>
<h1>Perturbing the states of Growing CAs</h1>
<p>We observed that it was hard to fool growing CAs into changing their morphology by placing viruses inside them. These adversaries had to devise complex and local behaviors that would cause the states of original cells nearby, and ultimately globally, to change their morphology.</p>
<p>In this section, we explore how we can circumvent this added difficulty of indirect behavioral change from local interactions, by performing global state changes.</p>
<p>We again focus on Growing CAs and we choose the lizard pattern again for illustrative purposes. Every cell of a growing CA consists of a group of 16 states, some of them phenotypical (RGBA states) and the remaining 12 of arbitrary purposes, generally used for storing and communicating information. As we emphasized above, because the states of every cell are used to understand what to do and communicate with each other, we can perturb the states of these cells to hijack the overall system in certain ways (discovery of such perturbation strategies is a key goal of biomedicine and synthetic morphology). There are a variety of ways we can perform state perturbations. We will focus on <i>global state perturbations</i>, defined as perturbations that are applied on every alive cell in every time step (analogous to "systemic" biomedical interventions, that are given to the whole organism (e.g., a chemical take internally), as opposed to highly localized delivery systems).. The new goal is to discover a certain type of global state perturbation that results in a stable new pattern.</p>
<figure>
<img src='images/drawing_template_lizard_perturbation.jpg' style='width: 650px'>
<figcaption>Diagram showing some possible stages for perturbing a lizard pattern. (a) We start from a seed that grows into a lizard (b). (c) We perform a global state perturbation at every step and the lizard loses its tail. (d) Empirically, when we stop perturbing the state we observe the lizard to grow back its tail.</figcaption>
</figure>

<p>We show 6 target patterns: the tailless and red lizard from the previous experiment, plus a blue lizard and lizards with other severed limbs and severed head.</p>
<figure>
<img src='images/mutations_mosaic.jpeg' style='width: 450px'>
<figcaption>Mosaic of the desired mutations we want to apply.</figcaption>
</figure>
<p>We decided to experiment with a very simple kind of global state perturbation: applying a symmetric $16\times16$ matrix multiplication $A$ to every alive cell at every step<d-footnote> In practice, we also clip the state of cells such that they stay bounded within $[-3, +3]$. This is a minor detail and it stabilizes results minimally. Everything discussed here applies even if we were to not clip the space.</d-footnote>. To give insight on why we chose this, an even simpler "state addition" mutation would be insufficient because our models are technically unbounded, and very often we would want to suppress something by setting it to zero - something impossible to do with constant state additions. However, matrix multiplications have the possibility of amplifying&#x2F;suppressing combinations of states, which is exactly what we would like to have. As to why we force the matrix to be symmetric will be clear later on.</p>

<p>We initialize $A$ as the identity matrix $I$ and train $A$ just like we would train traditional Growing CAs, with the following differences:</p>
<ul><li>We perform a global state perturbation through $A$ at every step.</li>
<li>The CA parameters are frozen and train $A$ only.</li>
<li>We consider initial image configurations to be both a seed and a fully grown lizard (as opposed to the Growing CA article, where initial configurations were seeds only).</li></ul>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mutations_mosaic_video.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of applying the trained perturbations.</figcaption>
</figure>


<p>The video above shows how this kind of training results in successfully discovering global state perturbations to change the target patterns to a desired variation of it. We show what happens when we stop perturbing the states (an out-of-training situation) at step 500 throughout step 1000, and then reapply the mutation. This way we observe how both from a seed and from a grown pattern applying these perturbations is successful, and that the original CAs easily recover from this state perturbation once it goes away. This last result is perhaps not surprising after we observed how robust growing CA models are in general.</p>

<p>We can also see how not all perturbations are equally effective. In particular, the headless perturbation is by far the hardest and it results in a loss of other details across the whole lizard pattern such as the white coloring on its back. We hypothesize that the best this simple perturbation managed to find was suppressing a "structure" that contained both the head and the white coloring. This may be related to the concept of differentiation and distinction of biological organs. Predicting what kinds of perturbations would be harder or impossible to be made before trying them out empirically is still an open research question. On the other hand, a variant of this kind of synthetic analysis might help with defining higher order structures within systems.</p>
<h2 id='directions-and'>Directions and compositionality of perturbations</h2>
<p>Let us go back to our choice of using a symmetric matrix for representing global state perturbations. Every complex symmetric matrix $A$ can be diagonalized as follows: </p>
<p>$A &#x3D; Q \Lambda Q^\intercal$</p>
<p>where $\Lambda$ is the diagonal eigenvalues matrix and $Q$ is the unitary matrix of its eigenvectors. Another way of seeing this now becomes as applying a change of basis transformation, scaling each single component proportional to the eigenvalues, and then changing back to the original basis. This should also give a clearer intuition on how easy it is to suppress&#x2F;amplify combinations of states. Moreover, we can now imagine what would happen if all the eigenvalues were to be one. In that case, we would naturally have $Q I Q^\intercal &#x3D; I$ resulting in a noop (the lizard would grow as if no perturbation would be performed). We now can decompose $Q \Lambda Q^\intercal &#x3D; Q (D + I) Q^\intercal$ where D is the <i>perturbation direction </i>($\Lambda - I$) in the "eigenvalue space". Suppose now we have a coefficient $k$ to scale D: $A_k &#x3D; Q (kD + I) Q^\intercal$. If $k&#x3D;1$, we have the original perturbation $A$ and $k&#x3D;0$, we have the noop $I$. One natural question would be whether we can explore this direction and discover meaningful perturbations. Lucky for us, </p>
<p>$A_k &#x3D; Q (kD + I) Q^\intercal &#x3D; k A + (1-k) I$ </p>
<p>so we do not even have to compute eigenvalues and eigenvectors and we can simply scale $A$ and $I$ accordingly.</p>

<p>Let us then take the tailless perturbation and see what happens as we vary k:</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/tail_direction_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of following the direction of the tail perturbation.</figcaption>
</figure>
<p>As we descend from $k&#x3D;1$ to 0 we can clearly see the tail forming more and more complete. Surprisingly, if we proceed towards negative k values, so to the opposite direction of what we trained for, the lizard grows a longer tail! Unfortunately, the further away we go, the more cancerogenous the system becomes and eventually the lizard explodes. This cancerogenic behaviour is likely due to the fact that some of the perturbations applied on the states are also responsible for changing the homeostatic regulation of the system, making some cells die out&#x2F;grow in different ways than before. When we go the opposite direction, we are both going towards a "longer tail" regimen, but also changing these regulatory processes and eventually cancer appears.</p>

<p><strong>Can we perform multiple individually trained perturbations at the same time?</strong> </p>
<p>Suppose we had two perturbations $A$ and $B$ and their eigenvectors were the same, or more realistically not that different from one another. Then, $A_k &#x3D; Q (k_A D_A + I) Q^\intercal$ and $B_k &#x3D; Q (k_B D_B + I) Q^\intercal$. </p>
<p>In that case, it could be that </p>
<p>$comb(A_k, B_k) &#x3D; Q(k_A D_A + k_B D_B + I)Q^\intercal &#x3D; k_A A + k_B B + (1 - k_A - k_B)I$ </p>
<p>would result in something meaningful. At the very least, if $A &#x3D; B$, setting $k_A &#x3D; k_B &#x3D; 0.5$ would result in exactly the same perturbation.</p>
<p>Since $D_A$ and $D_B$ are effectively a displacement from the identity $I$ and we have empirically observed how given any trained displacement $D_A$, for $0 \leq k_A \leq 1$ adding $k_A D_A$ results in a stable perturbation, we can hypothesize that as long as we have two perturbations whose positive directions $k$ are $k_A + k_B \leq 1$, this could result in a stable perturbation. Picture it as interpolating stable perturbations with the direction coefficients.</p>
<p>In practice the eigenvectors are also different so the combination results are likely going to be worse the more different the respective eigenvector bases are.</p>
<p>Now let us take a look at two perturbations, tailless and no leg lizards, and interpolate their direction coefficients by keeping their sums equal to 1:</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/lizard_combinations_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of composing two trained perturbations while keeping the sum of k=1.</figcaption>
</figure>
<p>We can see how it seems to work, but it has some side effects such as the whole pattern sliding up. Similar results happen with other combinations of perturbations. Now, what would happen if we removed the restriction of the sum of k equal to 1, and instead added both perturbations entirely? We know that if the two perturbations were the same, we would end up going twice as far away from the identity perturbation, and in general we expect the variance of these perturbations to increase. Effectively, this means going further and further away from the stable perturbations discovered during training. This should mean that there are more things that can go wrong the higher the sum of k are. This can be generalized to an arbitrary number of perturbation compositions.</p>
<p>We now show what happens when we combine the tailless and the no leg lizard perturbations at their fullest. Note that when we set both k to 1, the resulting perturbation is equal to the sum of the two perturbations minus an identity matrix.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="320px" preload="auto">
      <source src="images/combination_of_mutations.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of composing two trained perturbations.</figcaption>
</figure>
<p>Surprisingly, the resulting pattern is almost like we would desire it to be. It is evidently not perfect as it is sliding upwards even faster than the other combinations we tried, showing it's even less stable than them.</p>

<p>The arguments we used can be generalized to any arbitrary number of perturbations. We created a small playground that allows the reader to input their desired combinations. Empirically, we were surprised by how many of these combinations result in good perturbations and it appears true that keeping k bound to 1 results in generally more stable patterns. We also observed how going the opposite direction from a perturbation is usually much more unstable.</p>
<p>Below there is a video showing the playground and the playground underneath.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/perturbation_playground.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Demonstration of the playground.</figcaption>
</figure>

<iframe src="mutating_gca_demo.html" scrolling="no" frameborder="0"
    style="position: relative; height: 600px; width: 100%;"
></iframe>

<h1>Related work</h1>
<p>This work is inspired by Generative Adversarial Networks (GANs) <d-cite key="goodfellow2014generative"></d-cite>. While with GANs it is typical to cotrain pairs of models, in this work we froze the original CAs and trained the adversaries only. This is to the greatest degree inspired by the seminal work <i>Adversarial Reprogramming of Neural Networks </i><d-cite key="elsayed2018adversarial"></d-cite>.</p>
<p>The kinds of state perturbations performed in this article can be seen as targeted latent state manipulations. Word2vec <d-cite key="mikolov2013efficient"></d-cite> shows how latent vector representations can have compositional properties and it was of great inspiration to us.</p>
<h1>Discussion</h1>
<p>This article showed two different kinds of adversarial attacks that Neural CAs are susceptible to.</p>

<p>The adversarial injections of CAs in frozen Self-classifying MNIST digit CAs showed how a system of agents that are heavily reliant on the passing of information among each other is easily swayed by deceitful signaling. This problem is routinely faced by biological systems, which face hijacking of behavioral, physiological, and morphological regulatory mechanisms by parasites and other agents in the biosphere with which they compete.  Future work in this field of computer technology can benefit from research on biological communication mechanisms to understand how cells maximize reliability and fidelity of inter- and intra-cellular messages required to implement adaptive outcomes.  </p>
<p>[Social comment?]</p>

<p>This kind of attack was much less effective against Growing CAs. On the one end, this is a positive property to have for engineering systems. On the other hand, this dynamic is also of importance to the scaling of control mechanisms (swarm robotics and nested architectures): a key step in "multicellularity" (joining together to form larger systems from sub-agents <d-cite key="levin2019theComp"></d-cite>) is informational fusion, which makes it difficult to identify the source of signals and memory engrams. An optimal architecture would need to balance the need for validating control messages with a possibility of flexible merging of subunits, which wipes out metadata about the specific source of informational signals. Likewise, the ability to respond successfully to novel environmental challenges is an important goal for autonomous artificial systems, which may import from biology strategies that optimize tradeoff between maintaining a specific set of signals and being flexible enough to establish novel signaling regimes when needed.</p>

<p>The global state perturbation experiment on Growing CAs shows how it is still possible to hijack these CAs towards stable out-of-training configurations and how these kind of attacks are somewhat composable in a similar way to how embeddings spaces are manipulable in the natural language processing and computer vision fields [cite!]. However, this experiment failed to discover stable out-of-training configurations <i>after the perturbation was lifted</i>. [Biological comment on how this is not desirable.]</p>

</d-article>
<d-appendix>
    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
</d-appendix>
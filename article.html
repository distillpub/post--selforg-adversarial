<d-article>
<h1>Demo&#x2F;Video</h1>
<p>We show the adversarial MNIST demo. One problem here is that looking at the demo before reading anything will result in not knowing what to do there.</p>
<p>One option is to have a video explaining what is going on here, before showing the demo. Alternatively, here we have just a video, and the demo is presented at the end of the adversarial MNIST CA section.</p>
<h1>Introduction</h1>
<p>The Growing Neural Cellular Automata [cite] and Self-classifying MNIST Digits [cite] articles in this thread presented the Neural Cellular Automata (Neural CAs) model and showed how these models can be trained end-to-end differentiably to accomplish self-organising task such as pattern growth and digit self-classification. The resulting models were robust to various kinds of perturbations: the growing CAs expressed regenerative capabilities to damage; the MNIST CAs were responsive to change in the underlying digits, triggering reclassification whenever necessary.</p>

<p>In this work, we explore further the robustness of these models by <i>training adversaries </i>whose goal is to reprogram the CAs into doing something other than what they were trained to do. We will explore two kinds of adversarial computations: injecting some adversarial CA into the collective pretrained models; and perturbing the global state of the cells with some adversarial operation.</p>

<p>For the first type of adversarial attacks we will train a new CA model such that, when placed in an environment with the original models described in the previous articles, is able to hijack the behavior of the collective CAs. [Biological comment]</p>

<p>The second type of adversarial attacks will interact with previously trained growing CA models without touching any cell's parameters. Instead, we will apply a global state perturbation to all alive cells. This can be seen as inhibiting or enhancing combinations of state values, in turn hijacking proper communications among cells and within the cell's own states. [Biological comments]</p>
<h1>Adversarial MNIST CAs</h1>
<p>As a brief recap, the Self-classifying MNIST digits task consisted in placing CAs on a plane forming the shape of an MNIST digit. The cells had to communicate among themselves in order to achieve a total agreement as to which digit they formed.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_orig_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
The original model behavior on test data.</figcaption>
</figure>

<p>In this experiment, <strong>the goal is to create an adversarial CA that can hijack the global cells' classifications to always be an 8</strong>. We take the CA model trained on the previous article and freeze its parameters. We then train a new set of CAs whose model is identical to the specs of the frozen model but is randomly initialized. The training regime is mostly identical to the one used for training the self-classifying MNIST digits CAs. There are three important differences:</p>
<ul><li>Regardless of what the actual digit is, we consider <i>the correct classification to always be an 8</i>.</li>
<li>For each batch and each pixel, the CAs are randomly chosen to be either the pretrained model or the new adversarial one. We used a 10% chance of placing an adversarial CA instead of the pretrained one.</li>
<li>We train only the adversarial CA parameters and keep the pretrained model frozen.</li></ul>

<p>The task is remarkably simple to optimize, reaching convergence in as little as 2000 training steps. Here we show a few examples of test instances.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_10perc_adv_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 10% of the cells.</figcaption>
</figure>
<p>The adversaries are constantly communicating with their neighbors to keep them convinced of the wrong classification. We can see that by visualising what happens when we remove the adversaries after 200 steps. While some digits don't recover, most of them self-correct to the right classification.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_10perc_adv_reset_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
We remove the adversaries after 200 steps. Most digits recover, but not all.</figcaption>
</figure>
<p>While we trained the adversaries with a 10% distribution, we can observe that very often we need significantly fewer adversaries to steer the entire classification towards an 8. Here is a test run with 1% of cells being adversaries.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_1perc_adv_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 1% of the cells.</figcaption>
</figure>
<p>We created a demo playground where the reader can draw digits and place adversaries with surgical precision. We encourage the reader to play with the demo to get a sense on how easily innocent cells can get swayed towards the wrong outcomes. </p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mnist_adversary_playground.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Playground video to be replaced by the real playground.</figcaption>
</figure>
<h1>Adversarial Injections for Growing CAs</h1>
<p>One question we may ask is whether the same kind of adversarial attack that was successful for the MNIST CA would work for the Growing CA model too. The Growing CA models' goals are to be able to grow a complex image pattern from a small seed, and having its result being persistent over time and robust to significant perturbations. The drawing below exemplifies what just described.</p>
<p>[ Add drawing of: from lizard seed to pattern, to perturbation to regeneration]</p>

<p>In this article, we will focus on the lizard pattern.</p>
<figure>
<img src='images/lizard_complete_low_res.png' style='width: 225px'>
</figure>
<p>The new goal will be to have some adversarial CAs change the global configuration of all the CAs, including the previously trained and now frozen lizard CAs. We showcase two new targets for the adversaries to generate: a tailless lizard and a red lizard. </p>
<figure>
<img src='images/lizard_new_targets_exp2.png' style='width: 450px'>
<figcaption>List of the desired mutations we want to apply.</figcaption>
</figure>

<p>These targets have very different properties: converting a lizard from green to red would show a global change in the behaviour of the resulting CAs, while having a severed tail is a more localized change.</p>

<p>We first train adversaries for the tailless target with a 10% chance for each cell to be a virus. Note how we prohibit cells to be viruses if they are outside the target pattern: the tail contains no viruses.</p>

<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/lizard_no_tail_virus_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 10% of the cells.</figcaption>
</figure>

<p>The video above shows 6 different runs of the same model with different adversaries placements. The results vary considerably: sometimes the virus successfully gets rid of the tail, sometimes the tail is only shrunk but not completely removed, and other times the pattern becomes unstable. It is also important to note that training these adversaries required much longer times to achieve convergence and the acquired convergence is of a worse quality than what achieved for the MNIST CA adversarial experiment.</p>

<p>This difference in quality is exacerbated in trying to fit the red lizard pattern. In this case, using only 10% of adversaries results in a complete failure. Some readers may wonder whether the frozen CAs have the capability of producing red cells at all, since there is no red in the original target, and suspect this was an impossible task to begin with. However, we note that the frozen model knows how to make red, because there are instances where a cell has to color itself white, and therefore has to know how to generate both green, red, and blue.</p>

<p>To drive this point home, and to also test how harder it is to have a "red virus" hijack the lizard pattern, we increased the percentage of the virus to as much as 60% of the total number of cells.</p>

<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/lizard_red_virus_single_run.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Adversaries on 60% of the cells. At 500 steps, we stop the image and show only cells that are from the original model.</figcaption>
</figure>
<p>In the video above we can see how, at least at the beginning, 60% of adversaries are actually capable of coloring the lizard red. Take particular notice of the "step 500"<d-footnote> The still-image of the video is on step 500, and the video stops for a bit more than a second on step 500.</d-footnote>, where we mask out all the adversary cells and only show the original cells. There, we can see how quite some original cells are colored in red now. This is proof that the adversaries successfully managed to steer neighboring cells to color themselves red, where needed.</p>

<p>However, the model is far from perfect: if left be for a while, it diverges and new lizards spawn. Moreover, the percentage of cells, being 60%, make it so the adversaries are heavily dependent on having a majority of cells being adversaries, and if we were to place a smaller percentage of adversaries, such as 20-30%, the model would be unstable.</p>

<p>These mixed results are very interesting: a model of CA extremely similar to the one seen in MNIST CA shows a much greater resistance to adversarial perturbation than those of its cousin. This empirical difference is further puzzling by the fact that both models, during training, are asynchronous and exposed to sizeable perturbations, with the goal of making them both robust to out-of-training configurations.</p>

<p>One notable difference between the two models is that the MNIST CAs have to change their opinion (digit classification) based on the information that they receive from their neighbors because at any time the underlying digit may change in shape even if in a specific position there would be no apparent change. For instance, imagine the case of a "1" turning into a "7": from the point of view of the cells in the lower part of the digit, there is no change, yet the total digit is a 7 now. This may make the MNIST CAs more susceptible to individuals' signals rather than what happens in Growing CAs, where the totality of cells is responsible for generating a given shape and there is never a moment where cells have to reconfigure themselves for regenerating something different than before.</p>

<p>Following this hypothesis, we suspect that more general-purpose Growing CAs that have observed different target patterns during training are more likely to be susceptible to adversarial attacks.</p>
<h1>Perturbing the states of Growing CAs</h1>
<p>We now turn our attention to adversarial attacks that do not modify or substitute any CA parameters, but interact with the system with other means. We again focus on Growing CAs, and we choose the lizard pattern again for illustrative purposes. Every cell of a growing CA consists of a group of 16 states, some of them phenotypical (RGBA states) and the remaining 12 of arbitrary purposes, generally used for storing and communicating information. As we described in the introduction, because the states of every cell are used to understand what to do and communicate with each other, we can perturb the states of these cells to hijack the overall system in certain ways.</p>

<p>There are a variety of ways we can perform state perturbations. We will focus on <i>global state perturbations</i>, defined as perturbations that are applied on every alive cell in every time step. The new goal is to discover a certain type of global state perturbation that results in a stable new pattern. We show 6 target patterns: the tailless and red lizard from the previous experiment, plus a blue lizard and lizards with other severed limbs and severed head.</p>
<figure>
<img src='images/mutations_mosaic.jpeg' style='width: 450px'>
<figcaption>Mosaic of the desired mutations we want to apply.</figcaption>
</figure>
<p>We decided to experiment with a very simple kind of global state perturbation: applying a symmetric $16\times16$ matrix multiplication $A$ to every alive cell at every step<d-footnote> In practice, we also clip the state of cells such that they stay bounded within $[-3, +3]$. This is a minor detail and it stabilizes results minimally. Everything discussed here applies even if we were to not clip the space.</d-footnote>. To give insight on why we chose this, an even simpler "state addition" mutation would be insufficient because our models are technically unbounded, and very often we would want to suppress something by setting it to zero - something impossible to do with constant state additions. However, matrix multiplications have the possibility of amplifying&#x2F;suppressing combinations of states, which is exactly what we would like to have. As to why we force the matrix to be symmetric will be clear later on.</p>

<p>We initialize $A$ as the identity matrix $I$ and train A just like we would train traditional Growing CAs, with the difference that the CA parameters are frozen, that we perform a global state perturbation through $A$ at every step, and that we consider initial image configurations to be both a seed and a fully grown lizard (as opposed to the Growing CA article, where initial configurations were seeds only).</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/mutations_mosaic_video.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of applying the trained perturbations.</figcaption>
</figure>


<p>The video above shows how this kind of training results in successfully discovering global state perturbations to change the target patterns to a desired variation of it. We can also see how not all perturbations are equally effective: in particular, the headless perturbation is by far the hardest and it results in a loss of other details across the whole lizard pattern. We also show what happens when we stop perturbing the states (an out-of-training situation) at step 500 throughout step 1000, and then reapply the mutation. This way we observe how both from a seed and from a grown pattern applying these perturbations is successful, and that the original CAs easily recover from this state perturbation once it goes away. This last result is perhaps not surprising after we observed how robust growing CA models are in general.</p>
<h2 id='directions-and'>Directions and compositionality of perturbations</h2>
<p>Let us go back to our choice of using a symmetric matrix for representing global state perturbations. Every complex symmetric matrix $A$ can be diagonalized as follows: </p>
<p>$A &#x3D; Q \Lambda Q^\intercal$</p>
<p>Where $\Lambda$ is the diagonal eigenvalues matrix and </p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/tail_direction_mosaic.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of following the direction of the tail perturbation.</figcaption>
</figure>

<p>Show multiple mutations and playground.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/combination_of_mutations.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Effect of composing two trained perturbations.</figcaption>
</figure>
<p>Below there is a video showing the playground. It makes sense to actually just put the playground there instead.</p>
<figure>
    <div class="vc">
    <div class="vidoverlay"></div>
    <video playsinline muted width="640px" preload="auto">
      <source src="images/perturbation_playground.mp4#t=3.0" type="video/mp4">
Your browser does not support the video tag.
    </video>
    </div>
<figcaption style="">
Demonstration of the playground.</figcaption>
</figure>
<h1>Related work</h1>
<p>Both Adversarial key related work, and biological related work.</p>
<h1>Discussion</h1>
<p>Closing notes.</p>

</d-article>
<d-appendix>
    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
</d-appendix>